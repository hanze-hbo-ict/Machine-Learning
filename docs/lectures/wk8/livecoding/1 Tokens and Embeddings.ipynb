{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2247b3e6-9c71-4efb-b18e-8424584d6c8d",
   "metadata": {},
   "source": [
    "All Notebooks in this folder are based on the \"Hands-on Large Language Models\" book by Jay Alammar and Maarten Grootendorst, and the corresponding public repo.\n",
    "\n",
    "See https://www.llm-book.com/\n",
    "    \n",
    "The code has been tweaked to run on a Mac M4 with 10 GPU cores. Change ``device=\"mps\"`` to ``device=\"cuda\"`` when using Nvidia hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89118d47-1519-4710-820a-1870d1e8d6b9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5969869-24ae-449e-b9df-2a4e2b270fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "colors_list = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    print(f\"{tokenizer_name}:\", end=' ')\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee91123-883c-4ae4-801e-6e5863057837",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "ðŸŽµ é¸Ÿ\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd96ed-d23f-41ca-bf74-5371e386aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no newlines\n",
    "# all lower-case\n",
    "# UNK for unknown characters\n",
    "show_tokens(text, \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99dac5-ffa9-42aa-b68a-b700585ddf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no newlines\n",
    "# casing is kept intact\n",
    "# UNK for unknown characters\n",
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a687317-7fb8-43b6-8a9e-ee42c59ba3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses Byte Pair Encoding (BPE)\n",
    "# newlines and special characters are also encoded\n",
    "show_tokens(text, \"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611a5f2-dffd-4dd8-865b-699fce6ffe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses BPE\n",
    "# has special tokens for coding, like for 4 spaces and \"elif\"\n",
    "show_tokens(text, \"Xenova/gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95d1f8-aa42-4fa6-9884-e333bed6289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like gpt-4, but keeps digits separate to improve math skills\n",
    "show_tokens(text, \"bigcode/starcoder2-15b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec0ec6-057e-4e4b-96e2-335c6042b674",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b27410-a388-4f51-ab69-444b7b7f637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# Load a language model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "    for token in tokens['input_ids'][0]:\n",
    "        print(tokenizer.decode(token))\n",
    "\n",
    "    # Process the tokens\n",
    "    output = model(**tokens)[0]\n",
    "\n",
    "    print(output.shape)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764d6bc-e648-4836-9e94-72cf941dbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sentence('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b8595-7be4-4b72-afe4-93a16b458722",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sentence('Hello world') # embeddings should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a83fb8-2dc1-4652-9cad-1622787b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sentence('Hello Anna, how are you doing?') # embedding for Hello is now different because of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7829560-6e96-49db-b515-b8d177d3fa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
