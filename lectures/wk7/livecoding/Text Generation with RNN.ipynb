{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db9fffd-ccab-4f14-9947-1e2ededcd2e4",
   "metadata": {},
   "source": [
    "# Tekstgeneratie met een RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ccc82-815f-4076-a24e-6a1f9556a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ab7b9-264d-4a8b-bb73-3be7f29ee47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"moon.txt\") as f:\n",
    "    moon_text = f.read()\n",
    "\n",
    "print(moon_text[:250])\n",
    "\n",
    "moon_text = moon_text[:100_000] # Pak eerste 100.000 karakters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03927-3e2a-4e02-9dfb-fa20fab1ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tekst omzetten naar losse karakters en dan naar integers\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([moon_text])\n",
    "encoded = text_vec_layer([moon_text])[0]\n",
    "\n",
    "print(encoded[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5beb6f7-7044-4daf-bfe3-5fa6dfe46452",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba974a-8f0c-4c9c-9b78-91364b35e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2 # haal de padding- en unknown-karakters (0 en 1) eraf\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80eb84-e198-4a2c-b984-3fa0d20962ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    \"\"\" Maak inputs en targets. Beide zijn 'length' tekens lang.\n",
    "    Een target begint één karakter verder dan zijn input en eindigt ook één karakter verderop.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length+1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1)) # length+1 vanwege target\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1) # hier worden input en target gesplitst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4a862-6f9a-4228-b8e3-e0d8a9ea9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab99f3-b017-4f00-b5aa-7902d8633b0a",
   "metadata": {},
   "source": [
    "## Model maken en trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365a8ea-89ad-4762-80f5-005556a90602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test-split, 90% - 5% - 5%\n",
    "length=100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:90_000], length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[90_000:95_000], length=length, shuffle=True, seed=42)\n",
    "test_set = to_dataset(encoded[95_000:], length=length, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf3f32-69c1-4371-a3cd-0eafed8c6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16), # zet de inputs om naar 16-dimensionale word embeddings (zie hoorcollege)\n",
    "    tf.keras.layers.GRU(128, return_sequences=True), # laag met units met \"geheugen\"\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\") # outputlaag met n_tokens grootte, voorspelt dus één token (karakter), softmax zodat de kansen optellen tot 1\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]) # als je met one-hot-encoded data werkt, gebruik dan loss=categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6a837-8472-4125-97f2-734a58a0c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 90_000 // 32\n",
    "#history = model.fit(train_set, validation_data=valid_set, epochs=10, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f908f64-5149-4892-9375-bfdd8d3a5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"def_moon_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd92a05-8f01-44e8-b5ee-c95a33719dfb",
   "metadata": {},
   "source": [
    "## Voorspellingen doen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f0626-fcd4-4074-a8f3-6deb6602e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"def_moon_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c3c48-303b-420b-b378-29a5c38510a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2),\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd143bf1-7d6d-451b-aec1-c46647be4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = moon_model.predict(tf.constant([\"Human brai\"]), verbose=0)\n",
    "y_pred = tf.argmax(y_proba[0,-1])\n",
    "text_vec_layer.get_vocabulary()[y_pred+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed861dfb-dc20-473f-8e93-5107af57d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_probas = moon_model.predict(tf.constant([text]), verbose=0)[0,-1:]\n",
    "    rescaled_probas = tf.math.log(y_probas) / temperature\n",
    "    next_char_index = tf.random.categorical(rescaled_probas, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[next_char_index + 2]\n",
    "\n",
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be300fae-24b3-4678-a22f-e191cb9c08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ceacf4-b5f7-4041-9f17-a589c66f3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"Human brain has around ten-to-the-tenth neuron\", n_chars=200, temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954b1f9-6d34-4268-91f5-6ebf77ed098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"Human brain has around ten-to-the-tenth neuron\", n_chars=200, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8ed09-c94f-4bf9-997f-d5dd05b88c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"Human brain has around ten-to-the-tenth neuron\", n_chars=200, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefe683-073c-4059-aee1-e9a94a9f37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"Human brain has around ten-to-the-tenth neuron\", n_chars=200, temperature=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f739314-d299-409b-808f-98c570d7c453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
