{"config":{"lang":["nl"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welkom bij Machine Learning","text":"<p>Hier vindt u onder andere de stof (per week) en de opgaven (per blok, of \"deel\", van twee weken).</p>"},{"location":"index.html#inleiding","title":"Inleiding","text":"<p>Machine learning is het onderdeel van de informatica dat zich bezighoudt met het analyseren van grote hoeveelheden data en op basis daarvan structuren herkennen of voorspellingen doen. Hoewel de basis voor dit vakgebied al in de jaren zestig van de vorige eeuw is gelegd (bijvoorbeeld door het baanbrekende werk van Frank Rosenblatt, is het pas de voorbije twee decennia echt tot grote bloei gekomen. Dit heeft vooral te maken met de enorme hoeveelheid data die heden ten dage beschikbaar wordt gesteld (een fenomeen bekend onder de term information explosion in combinatie met de grote en goedkope computationele kracht van hedendaagse computers en data-centra.</p>"},{"location":"index.html#opgaven","title":"Opgaven","text":"<p>Op deze pagina's zijn de weekopgaven van dit onderdeel te vinden. Deze opgaven worden gemaakt in duo's. Tijdens het eerste practicum wordt klassikaal een begin gemaakt met de opgaven: er wordt uitgelegd hoe te werken met numpy en hoe je lineaire algebra gebruikt in het domein van ML. Ook worden hier de weekopgaven verder toegelicht.</p> <p>Telkens tijdens het tweede practicum van elke tweede week worden afspraken gemaakt met individuele duo's om het werk tot dan toe te bespreken, vragen te beantwoorden en opmerkingen te plaatsen. Deze gesprekken zijn verplicht. Tijdens deze gesprekken wordt ook het begrip van de materie getoetst. </p> <p>Er zijn vier sets opgaven die uiteindelijk in de voorlaatste week van periode 4.1 moeten zijn afgerond (je bent dus vrij om \u00e9\u00e9n en ander te plannen, zo lang je maar aan deze eis voldoet). Eventueel reparatiewerk wordt tijdens de gesprekken besproken. Mocht dat niet voor de deadline zijn afgerond, dan is er een uitloopmogelijkheid in de laatste week van de periode; dat geldt dan wel als een herkansing: als je hiervan gebruik maakt, kun je voor dit onderdeel maximaal een 6 halen. Voor het overige zijn de cijfers ONV, 6, 8 of 10.</p>"},{"location":"index.html#opstarten","title":"Opstarten","text":"<p>De opgaven voor elke week gaan uit van een zipje met startcode. Het geheel gaat uit van een aantal dependencies. Deze dependencies hebben we voor het gemak in een <code>requirements.txt</code> gezet. Je kunt het beste een virtuele omgeving aanmaken en hierin met pip in \u00e9\u00e9n keer alle dependencies installeren. Hier een voorbeeld voor MacOS:</p> <pre><code>baba@aurelia ~ % virtualenv ml\ncreated virtual environment CPython3.8.7.final.0-64 in 820ms\nbaba@aurelia ~ % cd ml \nbaba@aurelia ml % cp ~/Downloads/requirements.txt .\nbaba@aurelia ml % source bin/activate\n(ml) baba@aurelia ml % python -m pip install -r requirements.txt \n...\n(ml) baba@aurelia ml % \n</code></pre>"},{"location":"index.html#stof-en-opgaven-per-week","title":"Stof en opgaven per week","text":""},{"location":"index.html#deel-1","title":"Deel 1","text":"<ul> <li>Week 1</li> <li>Week 2</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-2","title":"Deel 2","text":"<ul> <li>Week 3</li> <li>Week 4</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-3","title":"Deel 3","text":"<ul> <li>Week 5</li> <li>Week 6</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-4","title":"Deel 4","text":"<ul> <li>Week 7</li> <li>Week 8</li> <li>Inlevermoment</li> </ul>"},{"location":"files/Opdracht%20model-evaluatie.html","title":"Opdracht model evaluatie","text":"<pre><code>from sklearn.datasets import load_breast_cancer\nimport numpy as np\n</code></pre> <pre><code># YOUR CODE HERE\n</code></pre> <p>Maak een Support Vector Classifier met de standaard-waarden voor alle parameters. Geef dit model mee aan <code>plot_learning_curve</code> die in <code>helpers.py</code> te vinden is. Behalve dit model verwacht die methode eveneens een titel, de <code>X</code> en de <code>y</code>. De volledige signature van die methode staat hieronder; bestudeer eventueel de code om de volledige implementatie te zien.</p> <pre><code>plot_learning_curve(\n    estimator,\n    title,\n    X,\n    y,\n    axes=None,\n    ylim=None,\n    cv=None,\n    n_jobs=None,\n    scoring=\"accuracy\",\n    train_sizes=np.linspace(0.1, 1.0, 5),\n)\n</code></pre> <pre><code>from sklearn.svm import SVC\nfrom helpers import plot_learning_curve\n# YOUR CODE HERE\n</code></pre> <p>Als het goed is, heb je nu hierboven drie grafieken staan. Bedenk op basis van deze visualisatie hoe goed of hoe slecht je vindt dat je classifier werkt.</p> <p>Experimenteer vervolgens met verschillende waarden voor de parameters van die <code>SVC</code>: verander de kernel en verhoog (als je kernel <code>poly</code> is) de <code>degree</code>.  Welke verschillen zie je in de visualisatis? Kun je op basis hiervan een voorstel doen voor de beste waarden voor die parameters?</p> <p>Maak gebruik van <code>train_test_split</code> om de data op te splitsen in tachtig procent trainingsdata en twintig procent testdata.</p> <p>Train een <code>SVC</code> op basis van de beste parameters die je hierboven hebt ge\u00efdentificeerd. Maak vervolgens een confusion matrix en een classificatie-raport op basis van de testdata met dit model. Maak tenslotte een ROC-curve van dit getrainde model. </p> <p>Geef op basis hiervan een analyse van de kwaliteit van het model en een advies over hoe het model eventueel te verbeteren zou zijn.</p> <pre><code>from sklearn.model_selection import train_test_split\n# YOUR CODE HERE\n</code></pre> <pre><code>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n# YOUR CODE HERE\n</code></pre> <pre><code># Plot een confusion-matrix.\n# Maak gebruik van de klasse ConfusionMatrixDisplay die hierboven is ge\u00efmporteerd\n# YOUR CODE HERE\n</code></pre> <pre><code># Plot een ROC-curve.\n# Maak gebruik van de klasse RocCurveDisplay die hierboven is ge\u00efmporteerd\n# YOUR CODE HERE\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# DataFrame om de gevonden metrieken per classifier in op te slaan.\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# YOUR CODE HERE\n</code></pre> <p>In de cel hieronder wordt de variabele <code>result_table</code> gebruikt om de verschillende ROC's in \u00e9\u00e9n figuur te plotten. Je hoeft hiervoor niks te programmeren; als je de cel runt krijgt je als het goed is direct de juiste visualisatie. </p> <p>Kun je op basis van deze visualisatie een uitspraak doen over welk model de beste performance heeft voor deze dataset? </p> <pre><code>import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=f\"{i}, AUC={result_table.loc[i]['auc']:.3f}\")\n\nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n</code></pre>"},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-model-evaluatie","title":"Opdracht model-evaluatie","text":""},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-1","title":"Opdracht 1","text":"<p>Laad de borstkanker-dataset en maak gebruik van <code>DESCR</code> om een beeld te krijgen van de gegevens die in deze dataset zijn opgeslagen. zorg ervoor dat je de features in een variabele <code>X</code> krijgt en de targets in de variabele <code>y</code> (dit kan op minimaal twee manieren). Je hoeft voor deze opgave geen EDA te maken of de data helemaal op te schonen (mag natuurlijk wel).</p> <p>Je hoeft het niet allemaal in \u00e9\u00e9n cel te doen; voel je vrij om meer cellen aan te maken wanneer je dat wilt.</p>"},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-2","title":"Opdracht 2","text":"<p>Maak en train nu verschillende andere typen classifiers (een aantal is hieronder gegeven, maar voel je vrij om een andere set te gebruiken). Let op: alle classifiers in sklearn implementeren dezelfde interface: maak hiervan gebruik in je realisatie.</p> <p>In de cel hieronder wordt een DataFrame <code>result_table</code> gedefinieerd. Het is de bedoeling dat je van alle classifiers die je gebruikt en traint de <code>fpr</code>, de <code>tpr</code> en de <code>auc</code> in dit DataFrame opslaat. Je kunt hiervoor gebruik maken van de sklearn-methoden <code>roc_curve</code> en <code>roc_auc_score</code>. </p>"},{"location":"files/intro%20notebook%20en%20sklearn.html","title":"Intro notebook en sklearn","text":"<p>Zoals tijdens het theoriecollege is toegelicht, maken we in deze cursus vaak gebruik van Jupyter Notebooks, een feitelijke standaard voor het rapid prototyping van machine learning projecten. Het grote voordeel van notebooks is dat je de documentatie (in markdown) direct tussen je runbare code hebt staan. Hoewel oorspronkelijk ontwikkeld voor Python zijn er inmiddels voor de meeste talen kernels ontwikkeld, zodat je ook Java, Go of PHP in notebooks kunt schrijven.</p> <p>Een tweede stap die we gaan zetten is het gebruikmaken van een bibliotheek om het zware werk voor ons over te nemen: scikit learn. In de volgende twee opgaven gaan we alle code zelf uitprogrammeren (met python en numpyr), maar in het echt maak je gebruik van deze bibliotheek: die is sneller en makkelijker en stelt je in staat om je te richten op het maken en beoordelen van modellen in plaats van het goed laten werken van feitelijk vrij triviale programmacode.</p> <p>E\u00e9n van de voordelen van sklearn is dat de meest gebruikte datasets standaard in deze bibliotheek zitten. Veel van de voorbeelden waar we de komende weken mee gaan werken, zul je hierin terugvinden.</p> <p>In deze opgave maken we gebruik van de California Housing dataset. Run de volgende cel om de methode te importeren die deze dataset kan laden. Bestudeer de documentatie om te weten te komen wat er in deze dataset is opgeslagen en hoe je vervolgens de data daadwerkelijk laadt. </p> <pre><code># https://stackoverflow.com/a/49174340\n# Haal de onderstaande regel uit het commentaar als je SSL-errors krijgt:\n# ssl._create_default_https_context = ssl._create_unverified_context\nfrom sklearn.datasets import fetch_california_housing\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <p>Gebruik de onderstaande cel om de methode <code>fetch_california_housing</code> aan te roepen. Mocht je bij het laden SSL-errors krijgen, probeer dan de eerste regel in de bovenstaande cel uit het commentaar te halen en run die cel nogmaals. Gebruik <code>feature_names</code> om de namen van de eigenschappen van de dataset te weten te komen. Zorg ervoor dat je de data van het resultaat in een variabele <code>X</code> zet, en de target in een variabele <code>y</code>.</p> <pre><code># YOUR CODE HERE\n</code></pre> <p>Zoals altijd maken we ook een paar visualisaties van de data om een beeld te krijgen van wat er zoal in zit. We beginnen met een scatter-plot; alleen dit keer plotten we niet de \\(y\\)-vector tegen een eigenschap uit de \\(X\\)-matrix; omdat we weten dat we te maken hebben met geografische data, is het leuker om de lengte- en breedtegraden tegenover elkaar te plotten. Maar gebruik van <code>matplotlib.pyplot.scatter</code> om deze twee gegevens (Longitude en Latitude, respectievelijk) te plotten. Als je het goed hebt gedaan, kun je in de resulterende plot de kustlijn van Californi\u00eb herkennen.</p> <pre><code>#YOUR CODE HERE\n</code></pre> <p>Zoals je in de documentatie hebt gelezen, is de target-value de gemiddelde waarde van de huizen in die omgeving, uitgedrukt in honderdduizend dollar. Natuurlijk moeten we wat statistieken uit deze target-vector halen. Vul onderstaande cel aan, zodat de juiste waarden worden afgedrukt. Maar vervolgens gebruik van pyplot.hist om een histogram van deze data te plotten. Beargumenteer op basis van de statistische gegevens in hoeveel <code>bins</code> je deze histogram moet onderverdelen.</p> <pre><code>import statistics as st\n# YOUR CODE HERE\n# vervang '0' door de juiste code\nmin_value = 0\nmax_value = 0\nstdev = 0\ngemiddelde = 0\n\nprint ('==== DATA UIT DE TARGET-VECTOR ====')\nprint (f'Gemiddelde: {gemiddelde:&amp;gt;10.2f}')\nprint (f'Minimum: {min_value:&amp;gt;10.2f}')\nprint (f'Maximum: {max_value:&amp;gt;10.2f}')\nprint (f'StdDev: {stdev:&amp;gt;10.2f}')\n</code></pre> <p>Een belangrijke stap om een beeld te krijgen van de data in de set is door gebruik te maken van een histogram. E\u00e9n van de belangrijke vragen daarbij is in hoeveel <code>bins</code> je de data moet verdelen. Daarvoor zijn grofweg twee methoden: Sturge's Rule en Freedman-Diaconis rule. Bestudeer deze blog hierover maak beide histogrammen. Let op dat het aantal <code>bins</code> een geheel getal moet zijn.</p> <p>Als het goed is, kom je in het eerste geval op 16 <code>bins</code> en in het tweede geval op 46. Welke van beide histogrammen vind je beter en waarom?</p> <pre><code># histogram met Sturge's Rule\nm,n = X.shape\n\n# YOUR CODE HERE\n</code></pre> <pre><code># histogram met Freedman-Diaconis rule\nm,n = X.shape\n\n# YOUR CODE HERE\n</code></pre> <p>Nu gaan we de features van deze dataset gebruiken om een voorspelling te doen van de waarde van een huis. We gaan in de volgende opgave de wiskunde helemaal zelf uitprogrammeren, zodat je weet wat er exact gebeurt. Vooralsnog maken we gebruik van 'sklearn.linear_model.linear_regression'.</p> <p>Verdeel de data in 20% testdata en 80% trainingsdata. Maak hiervoor gebruik van <code>train_test_split</code>. Laad de data opnieuw in met de parameter <code>return_X_y</code> op <code>True</code>, zodat je direct de features en de corresponderende targets hebt. Waarom is deze split ook al weer nodig?</p> <p>Gebruik vervolgens de methode <code>fit</code> om het model te trainen. </p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n#YOUR CODE HERE\n</code></pre> <p>Gebruik nu de methode <code>predict</code> om op basis van de test-data een uitspraak te doen over hoe goed het model presteert. Gebruik hiervoor de methode <code>mean_square_error</code> uit <code>sklearn.metrics</code>. Hoe vind je dat het model presteert? Wat zou je kunnen doen om het model te verbeteren?</p> <pre><code>from sklearn.metrics import mean_squared_error\n#YOUR CODE HERE\n</code></pre> <p>Bestudeer tenslotte met behulp van het <code>coef_</code>-attribuut van het getrainde model om te weten te komen wat de formule is die het model gebruikt. Welke features zijn het belangrijkst en welke minder?</p> <pre><code>#YOUR CODE HERE\n</code></pre> <pre><code>\n</code></pre>"},{"location":"files/intro%20notebook%20en%20sklearn.html#introductie-jupyter-notebook-en-scikit-learn","title":"Introductie Jupyter Notebook en Scikit-learn","text":""},{"location":"files/intro%20notebook%20en%20sklearn.html#opdracht-1-data-laden-en-inspecteren","title":"Opdracht 1: data laden en inspecteren","text":""},{"location":"files/intro%20notebook%20en%20sklearn.html#opdracht-2-lineaire-regressie","title":"Opdracht 2: Lineaire regressie","text":""},{"location":"week1/index.html","title":"Stof: Tools en technieken","text":""},{"location":"week1/index.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Machine Learning: achtergrond, geschiedenis, termen en technieken</li> <li>Jupyter Noteboook</li> <li>Handige packages: Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn</li> <li>Hele cyclus doorlopen a.d.h.v. de Titanic-dataset: data opschonen, prepareren, model fitten, evaluatie</li> </ul>"},{"location":"week1/index.html#college-sheets","title":"College-sheets","text":"<p>Na afloop van het hoorcollege kunt u hier de sheets vinden.</p>"},{"location":"week1/index.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 1 t/m 4 (ter voorbereiding)</li> <li>G\u00e9ron: hoofdstuk 1 en 2 (optioneel)</li> </ul>"},{"location":"week2/index.html","title":"Stof: Lineaire regressie","text":"<ul> <li>Lineaire regressie</li> <li>De kostenfunctie en stapsgewijze minimalisatie daarvan middels gradient descent</li> </ul>"},{"location":"week2/inleveren.html","title":"Inleveren deel 1","text":"<ul> <li>Maak de Notebook over Scikit Learn en de California Housing-dataset</li> <li>Maak de opgavenset over lineaire regressie</li> </ul>"},{"location":"week2/opgave1-2.html","title":"Deel 1 - opgaveset 2","text":""},{"location":"week2/opgave1-2.html#inleiding","title":"Inleiding","text":"<p>Deze week werken we aan de lineaire regressie van data met \u00e9\u00e9n variabele. De data die we daarvoor gebruiken is de winst van een vervoerder gerelateerd aan de grootte van een stad waar die vervoerder werkzaam is. Je kunt je voorstellen dat het nuttig is om deze verhouding te weten, omdat je op basis hiervan ge\u00efnformeerd een besluit kunt nemen of je in een nieuwe stad (met een bepaalde grootte) een nieuw filiaal wilt openen.</p> <p>De startcode van deze opgave kun je hier downloaden. Het bestand <code>week1_data.pkl</code> bevat de data waar we mee gaan werken. Dit is een 97\u00d72-numpy-vector: de eerste kolom bevat de grootte van de steden, de tweede kolom bevat de winst van de vervoerder.</p> <p>Daarnaast vind je twee python-bestanden in de zip. Het bestand <code>exercise1.py</code> gebruikt de code in <code>uitwerkingen.py</code> om door de opgaven te lopen. Het is de bedoeling dat je het bestand <code>uitwerkingen.py</code> afmaakt. Bestudeer de code en het commentaar in beide bestanden om te zien hoe ze werken en wat de bedoeling ervan is.</p>"},{"location":"week2/opgave1-2.html#1-het-visualiseren-van-de-data","title":"1. het visualiseren van de data","text":"<p>Een eerste stap in elk machine-learning project is een beeld cre\u00ebren van de data. Het eenvoudigst om dit te doen is door gebruik te maken van de library <code>matplotlib</code>. Hierin vind je een API <code>pyplot</code> waarmee je vrij eenvoudig een scatter plot kunt maken.</p> <p>Bestudeer de documentatie van <code>pyplot</code> en implementeer aan de hand hiervan de functie <code>draw_graph</code> in <code>uitwerkingen.py</code>, zodat je een afbeelding zoals de onderstaande krijgt. Hoewel de data niet echt normaal verdeeld is, is er wel een zekere verhouding waar te nemen tussen de grootte van de stad en de winst die de vervoerder maakt. In de rest van deze opgave gaan we deze verhouding bepalen.</p> <p></p>"},{"location":"week2/opgave1-2.html#2-het-bepalen-van-de-cost-function-de-kostenfunctie","title":"2. Het bepalen van de cost function (de kostenfunctie)","text":"<p>Zoals in de theorieles besproken is, is het bepalen van de verhouding tussen twee (of meer) grootheden een kwestie van het minimaliseren van de kostenfunctie: de som van de gekwadrateerde fouten (SSE). Dit minimaliseren doe je door middel van gradient descent en het is dikwijls nuttig om die kostenfunctie gedurende de iteraties bij te houden, zodat je de verschillende uitkomsten door je data heen kunt plotten \u2013 zo kun je bijvoorbeeld controleren of je niet in een lokaal minimum terecht bent gekomen. In deze opgave programmeren we de kostenfunctie verder uit; de volgende opgave gaat verder in op de gradient descent.</p> <p>De kostenfunctie is gegeven door de volgende formule:</p> \\[  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \\] <p>Hierin is \\(J(\\theta)\\) de totale kost die berekend wordt met de huidige waarden van \\(\\theta\\). Verder is \\(h_{\\theta}(x)\\) de hypothese van de waarde (de voorspelling) en is \\(y\\) de actuele (daadwerkelijke) waarde. Door het verschil tussen deze twee waarden voor elk datapunt op te tellen en uiteindelijk uit te middelen, komen we op de voorspellende waarde die de formule heeft met de huidige waarden van \\(\\theta\\).</p> <p>De algemene formule voor de hypothese (de voorspelling) is als volgt: </p> \\[ h_\\theta(x) = \\theta^T\\vec{x} = \\theta_0 + \\theta_1x  \\] <p>Omdat we op zoek zijn naar een lijn, hebben we feitelijk te maken met \u00e9\u00e9n parameter (een lijn is immers \\(y=ax + b = b + ax\\)). Allereerst bepalen we het aantal datapunten (\\(m\\)) en het aantal eigenschappen (features, \\(n\\)). Om de dimensionaliteit van de trainingsdata te laten corresponderen met \\(\\theta\\), voegen we vervolgens een rij van enen toe. Daarna isoleren we de laatste kolom van de data om de gewenste waarden te krijgen (de vector \\(y\\)); de rest van de data vormt dan de eigenlijke matrix \\(X\\). Tenslotte initi\u00ebren we de vector \\(\\theta\\):</p> <pre><code>m,n = data.shape\nX = np.c_[np.ones(m), data[:, [0]]]\ny = data[:, [1]]\ntheta = np.zeros( (2, 1) )\n</code></pre> <p>De voorspelde waarden in \\(X\\), de actuele waarden in \\(y\\) en een \\(theta\\) worden aan de methode <code>compute_cost</code> meegegeven; deze functie moet de waarde van \\(J(\\theta)\\) teruggeven. Implementeer deze functie op basis van de beschrijving hierboven in het bestand <code>uitwerkingen.py</code> (zie ook de aanwijzingen in het bestand zelf); maak hem zo, dat hij werkt voor elke grootte van <code>theta</code>. Maak gebruik van een vectori\u00eble implementatie.</p> <p>Als je klaar bent, kun je het bestand <code>opgaven.py</code> runnen. Dit bestand roept `compute_cost aan en print de gevonden waarde uit. Als het goed is, krijg je uiteindelijk een waarde van rond de 32.07.</p>"},{"location":"week2/opgave1-2.html#3a-gradient-descent","title":"3a. Gradient descent","text":"<p>Als laatste maak je de methode <code>gradient_descent</code> in het bestand <code>uitwerkingen.py</code> af. In deze methode wordt een aantal stappen uitgevoerd, waarbij in elke stap de vector \\(\\theta\\) volgens de onderstaande formule wordt aangepast.</p> \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\] <p>Als het goed is, zorgt elke stap in deze methode ervoor dat \\(J(\\theta)\\) lager wordt. Let er wel op dat je alle \\(\\theta_j\\) tegelijkertijd aanpast (in dit geval is de grootte van \\(\\theta\\) 2, dus elke iteratie moeten er twee parameters worden aangepast). Let er verder op dat je alleen \\(\\theta\\) aanpast: X en y zijn constante waarden die niet hoeven (of kunnen) te worden aangepast.</p> <p>De algemene structuur in het bestand is al gegeven. Als je implementatie klaar is, kun je opnieuw het bestand <code>opgaven.py</code> aanroepen; deze roept de functie <code>gradient_descent</code> aan, zodat de update 1500 keer wordt gedaan. Als het goed is, is \\(\\theta\\) uiteindelijk rond de (-3.63, 1.16).</p>"},{"location":"week2/opgave1-2.html#3b-kosten-bijhouden","title":"3b. kosten bijhouden","text":"<p>Omdat we, zoals gezegd, graag willen weten of de totale kost tijdens de gradient descent wel steeds minder wordt, is het van belang deze kosten tijdens deze descent bij te houden. Breid de functie <code>gradient_descent</code> aan zodat bij elke iteratie de functie <code>compute_cost</code> wordt aangeroepen met de huidige waarden van <code>theta</code>. Deze kosten stop je in de lijst <code>costs</code> (die al in de basiscode is ge\u00efnitialiseerd) en die retourneer je uiteindelijk eveneens (<code>gradient_descent</code> retourneert dus twee waarden).</p> <p>Maak vervolgens de functie <code>draw_costs</code> in <code>uitwerkingen.py</code>. Deze functie moet de lijst meekrijgen die je hierboven in <code>gradient_descent</code> hebt gevuld. Maak gebruik van <code>pyplot</code> om deze lijst in een grafiekje te zetten. Als je dan het bestand <code>exercise.py</code> runt, krijg je als het goed is iets als het onderstaande plaatje:</p> <p></p>"},{"location":"week2/opgave1-2.html#4-contour-plot","title":"4. Contour plot","text":"<p>In deze laatste opgave gebruik je de methode <code>compute_cost</code> die je hierboven hebt gemaakt om een contour-plot van de kosten te tekenen. Hierdoor kun je inzicht krijgen in hoe deze waarde zich ontwikkelt bij verschillende waarden van \\(\\theta\\). </p> <p>Het grootste deel van deze opgave is al in de methode <code>contour_plot()</code> in het bestand <code>uitwerkingen.py</code> gegeven; je hoeft alleen maar de waarden van de matrix <code>J_val</code> te vullen. Bestudeer het commentaar in het bestand voor meer toelichting. Als je klaar bent, roept het bestand <code>exercise1.py</code> de methode <code>contour_plot()</code> aan om de plot te tekenen. Als het goed is, ziet deze er ongeveer als hieronder uit.</p> <p></p>"},{"location":"week3/index.html","title":"Stof: Logistische regressie","text":"<ul> <li>Classicatie-problemen</li> <li>Logistische regressie</li> </ul>"},{"location":"week4/index.html","title":"Stof: Neurale netwerken","text":"<ul> <li>Neurale netwerken</li> <li>Tensorflow en Keras</li> </ul>"},{"location":"week4/inleveren.html","title":"Inleveren deel 2","text":"<ul> <li>Losse opgave logistische regressie: nog maken</li> <li>Maak de opgavenset over neurale netwerken</li> </ul>"},{"location":"week4/opgave2-2.html","title":"Deel 2 - opgaveset 2","text":""},{"location":"week4/opgave2-2.html#inleiding","title":"Inleiding","text":"<p>Deze en de volgende week staan in het teken van standaard datasets. Deze week werken met de zogenaamde MNIST dataset: een set van zevenduizend gray scale afbeeldingen van cijfers en letters geschreven door middelbare scholieren. Werken met de MNIST dataset is te vergelijken met de hello world van machine learning: vroeg of laat krijg je ermee te maken. Deze week programmeren we zelf een neuraal netwerk aan de hand van reeds geleerde gewichten; in de laatste week zullen we een framework gebruiken in een poging een andere dataset te classificeren.</p> <p>De set die we in deze week gebruiken is een subset van de oorspronkelijke dataset. Het gaat om vijfduizend samples, waarbij elk sample een plaatje van 20 bij 20 pixels is dat een getal van 0 tot 9 representeert. Elke kolom van deze 20 \u00d7 20 matrix is onder de vorige geplakt, zodat er uiteindelijke een 400 \u00d71 vector ontstaat. Deze vectoren zijn weer getransponeerd, zodat onze dataset \\(X\\) uiteindelijk een 5000 \u00d7 400 matrix is.</p> \\[ X = \\begin{bmatrix} &amp; \u2014 &amp; (x^{(1)})^T &amp; \u2014 &amp; \\\\ &amp; \u2014 &amp; (x^{(2)})^T &amp; \u2014 &amp; \\\\ &amp; &amp; \\vdots &amp; &amp; \\\\ &amp; \u2014 &amp; (x^{(m)})^T &amp; \u2014 &amp; \\\\ \\end{bmatrix} \\] <p>In deze matrix is $x^{(1)}$ de vector van het eerste plaatje, $x^{(2)}$ de vector van het tweede plaatje, enzovoort.  Behalve deze X-matrix is er in de data ook een 5000\u00d71 vector y gegeven, waarin per plaatje is aangegeven welk cijfer dit representeert. Om problemen met de 0 te voorkomen, is in deze vector 0 weergegeven als 10.</p> <p>De startcode van deze week is hier te downloaden. Opnieuw wordt deze opgave doorlopen door het script <code>exercise2.py</code>. Dit script importeert de functies uit <code>uitwerkingen.py</code> en runt die op volgorde. Het is de opgave om deze uitwerkingen af te maken. Bestudeer beide scripts om een idee te krijgen van de werking.</p>"},{"location":"week4/opgave2-2.html#1-het-visualiseren-van-de-data","title":"1. het visualiseren van de data","text":"<p>Zoals altijd gaan we eerst de data visualiseren. In dit geval betekent dat een vector uit \\(x\\) weer transformeren in een 20\u00d720 gray scale plaatje. Maak hiervoor de functie <code>plotNumber()</code> af. Omdat je weet dat de vector \\(x^{(i)}\\) het i-de plaatje uit de dataset representeert, kun je deze vector eenvoudig weer terug omzetten in een 20\u00d720 matrix en die tekenen. Maak daarbij gebruik van de methode <code>numpy.reshape</code> en van <code>matplotlib.matshow</code>. Het script roept de methode <code>plotNumber</code> aan met een willekeurige vector uit de matrix X. Het toont het nummer op de console, en het cijfer dat het plaatje zou moeten voorstellen. Op die manier kun je eenvoudig checken of je uitwerking correct is.</p> <p></p> <p>Als je deze opgave hebt afgerond, kun je het script aanroepen met <code>skip</code> als command line parameter: het tekenen wordt dan overgeslagen.</p>"},{"location":"week4/opgave2-2.html#2-het-neurale-netwerk-forward-propagation","title":"2. het neurale netwerk - forward propagation","text":"<p>Het neurale netwerk dat we voor deze week gaan uitprogrammeren bestaat uit drie lagen. De input van het netwerk zijn de 20\u00d720 plaatjes van de handgeschreven cijfers, dus de input-laag bestaat uit 400 nodes. De middelste verborgen laag heeft 25 nodes en de output-laag heeft tien nodes \u2013 \u00e9\u00e9n voor elk cijfer van nul tot en met negen. Gegeven een bepaalde input moet \u00e9\u00e9n van de tien output nodes de hoogste waarde hebben.</p> <p></p>"},{"location":"week4/opgave2-2.html#2a-de-sigmoid-functie","title":"2a: de sigmoid functie","text":"<p>Het bepalen van het cijfer dat door het plaatje wordt gerepresenteerd is feitelijk een classificatie probleem: de input vector \\(x^{(i)}\\) moet immers geclassificeerd worden als \u00e9\u00e9n van de cijfers 0 tot en met 9. Zoals tijdens de theorieles is besproken, hebben we voor dergelijke problemen de sigmoid functie \\(g(z)\\) nodig. De formule daarvan is als volgt:</p> \\[ g(z) = \\frac{1}{1+e^{-z}} \\] <p>Implementeer de methode <code>sigmoid</code> in <code>uitwerkingen.py</code>. Maak hem zo, dat je er zowel een getal als een vector aan kunt meegeven. In het eerste geval moet de functie de sigmoid-waarde van het getal retourneren, in het tweede geval moet hij een vector retourneren met de sigmoid-waarde van elk individueel element in de input-vector. Je kunt gebruik maken van de numpy-functie <code>exp()</code>.</p> <p>Als je klaar bent, kun je het script <code>exercise2.py</code> runnen. Deze roept de methode vijf keer aan: drie keer met individuele getallen -10, 0, en 10; vervolgens met deze drie getallen als een kolomvector en als een rijvector.</p>"},{"location":"week4/opgave2-2.html#2b-omzetten-van-een-vector-naar-een-matrix","title":"2b: omzetten van een vector naar een matrix","text":"<p>We beginnen met het omzetten van de rijvector \\(y\\) naar een matrix. Voor het berekenen van de kost van het netwerk moeten we namelijk deze vector omzetten in een 5000\u00d710 matrix van enen en nullen, waarbij het i-de element 1 is en de rest 0. Als bijvoorbeeld het label in \\(y\\) gelijk is aan 5, dan moeten we deze rij omzetten in een 10-dimensionale vector met een 1 op positie 5 en een 0 op de rest. Omdat de 0 in de y-vector gerepresenteerd wordt als 10, moet deze in de resulterende matrix een 1 krijgen op de eerste positie (met index 0).</p> \\[ y=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, ... , of \\begin{bmatrix}0\\\\0\\\\0\\\\\\vdots\\\\1\\end{bmatrix} \\] <p>Maak de methode <code>get_y_matrix()</code> in <code>uitwerking.py</code> af. Hierbij kun je gebruik maken van de methode csr_matrix uit scipy om op basis van y de matrix <code>y_vec</code> te maken. Let er daarbij wel op dat de y-vector 1-based is, terwijl de resulterende matrix 0-based moet zijn. Zie het onderstaande code-fragment voor een voorbeeld (je kunt ook gebruik maken van de methode todense() om een zogenaamde ijle matrix te maken):</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.sparse import csr_matrix\n&gt;&gt;&gt; cols = np.array([ 2,1,3,5 ])\n&gt;&gt;&gt; rows = [i for i in range(len(cols))]\n&gt;&gt;&gt; data = [1 for _ in range(len(cols))]\n&gt;&gt;&gt; width = max(cols) + 1 # arrays zijn zero-based\n&gt;&gt;&gt; y_vec = csr_matrix((data, (rows, cols)), shape=(len(rows), width+1)).toarray()\n&gt;&gt;&gt; y_vec\narray([[0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0]])\n&gt;&gt;&gt; \n</code></pre> <p>Als je klaar bent, kun je het script <code>exercise2.py</code> opnieuw runnen om de geretourneerde waarde te controleren.</p>"},{"location":"week4/opgave2-2.html#2c-voorspel-het-getal-en-bepaal-de-kost-van-deze-voorspelling","title":"2c. voorspel het getal en bepaal de kost van deze voorspelling.","text":"<p>Implementeer nu de methode <code>predict_number</code>. Maak hierbij gebruik van het stappenplan dat is gegeven in de afbeelding van het netwerk hierboven, en van het commentaar in de opgave. Let er bij je implementatie op dat de matrix <code>X</code> de waarden voor de input-nodes als rij bevat (400 rijen), terwijl de matrices \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) de waarde voor elke node als kolom bevatten (zo is \\(\\Theta^{(1)}_{2,3}\\) het gewicht tussen de derde input-node en de tweede verborgen node). Voorzie de data van de juiste indices en transpositioneer de matrix waar dat nodig is. De output van deze methode is een 10 &amp;times 1 vector die voor elk getal de waarschijnlijkheid dat de input dat getal is weergeeft.</p> <p>Om de accuratesse van deze voorspelling te bepalen, moeten we deze voorspelling vergelijken met de betreffende regel uit de y_matrix die we in het eerste deel van deze opgave hebben gemaakt. Dat gebeurt in de methode <code>compute_cost()</code>. De formule voor de kost is als volgt:</p> \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k\\right] \\] <p>Als je deze drie methoden hebt ge\u00efmplementeerd, kun je het script <code>exercise2.py</code> opnieuw aanroepen. Hier wordt begonnen met min of meer willekeurige waarden van de Theta's. We zetten deze niet op 0 (waarom niet?), maar verdelen we uniform in de range \\([-0.12, 0.12]\\) \u2013 zie hiervoor de methode <code>initialize_random_weights</code> in het script. Deze waarde is gebaseerd op het aantal nodes in het netwerk en wordt benaderd door </p> \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L{out}}} \\] <p>waarbij \\(L_{in}\\) en \\(L_{out}\\) staan voor het aantal nodes links en rechts van de betreffende laag. Het script voert met deze waarden van de matrices een forward propagation stap uit en toont de kost die correspondeert met de huidige waarden van de beide <code>Theta</code>'s. Als het goed is, ligt dit zo rond de 7 (de exact waarde is natuurlijk moeilijk te voorspellen). Vervolgens wordt de huidige accuratesse van het netwerk getoond (die is vanzelfsprekend extreem laag).</p>"},{"location":"week4/opgave2-2.html#3-het-neurale-netwerk-backpropagation","title":"3. het neurale netwerk \u2013 backpropagation","text":""},{"location":"week4/opgave2-2.html#3a-de-sigmoidegradient","title":"3a. de sigmo\u00efdegradi\u00ebnt","text":"<p>Om de relatieve bijdrage van een node te bepalen, hebben we de afgeleide van de sigmo\u00efdefunctie nodig. Deze is hieronder gegeven.</p> \\[ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) \\] <p>Implementeer deze afgeleide in de methode <code>sigmoid_gradient</code>. Zorg er opnieuw (net als bij de sigmo\u00efde zelf) voor dat deze methode zowel met scalaire waarden als met vectoren kan werken. Om zeker te weten of het goed is gegaan, roept het script <code>exercise.py</code> deze methode weer aan met drie verschillende waarden.</p>"},{"location":"week4/opgave2-2.html#3b-backpropagation","title":"3b. backpropagation","text":"<p>e gaan nu de backpropagation uitprogrammeren in de methode <code>nn_check_gradients</code>. De intu\u00eftie hierbij is als volgt: voor een observatie \\((x^{(i)}, y^{(i)})\\) doen we eerst een forward-propagation stap, waarbij we de voorspelling \\(a_3 = h_\\Theta(x^{(i)})\\) uitrekenen. Als we die hebben bepaald, willen we voor elke node in elke laag in het netwerk bepalen wat de bijdrage van deze node aan de totale fout is geweest. Deze fout geven we weer met \\(\\delta_j^{(l)}\\), waarbij \\(l\\) de laag en \\(j\\) het nummer van de betreffende node is.</p> <p>Voor de output-laag is de fout redelijk rechttoe-rechtaan te bepalen: dit is per node het verschil tussen diens output en de gewenste output (zoals opgeslagen in de matrix <code>y_vec</code>). </p> <p>Voor de verborgen nodes is dit iets complexer: de fout in laag \\(l\\) wordt berekend aan de hand van de gemiddelde fout in de laag \\(l+1\\). Zie het stappenplan hieronder; we adviseren je om de backpropagation uit te programmeren in een for-lus (<code>for i in range (m):</code>), omdat een vectori\u00eble implementatie complex is en een na\u00efeve implementatie waarschijnlijk leerzamer.</p> <p>Stap 1</p> <p>Gegeven een input \\(a^{(i)}\\), doe een standaard forward-propagation door gebruik te maken van de code uit <code>predict_number</code>; je moet de code hier wel herhalen, omdat je de verschillende waarden nodig hebt tijdens de backpropagation.</p> <p>Stap 2</p> <p>Zet voor elke output-node <code>k</code> in de derde laag </p> \\[ \\delta_k^{(3)} = (a_k^{(3)} - y_k) \\] <p>Stap 3</p> <p>Voor elke node in de verborgen (tweede) laag bepaal je diens 'bijdrage' aan de totale fout door het inproduct van de fout en de matrix (element wise) te vermenigvuldigen met de afgeleide van de sigmo\u00efdefunctie (die we in het eerste deel van deze opgave hebben gemaakt).</p> \\[ \\delta^{(2)} = (\\Theta^{(2)})^T\\cdot\\delta^{(3)} \\times g'(z^{(2)}) \\] <p>Stap 4</p> <p>Deze bijdrage tellen we op bij de andere bijdragen; op deze manier cre\u00ebren we twee nieuwe matrices \\(\\Delta^{(1)}\\) en \\(\\Delta^{(2)}\\), die dezelfde dimensionaliteit hebben als \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\).</p> \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}\\cdot(a^{(l)})^T \\] <p>Stap 5</p> <p>Als we deze stappen voor alle \\(m\\) observaties hebben gedaan, delen we de beide delta-matrices door het aantal observaties (element wise) om de gemiddelde fout per node voor de huidige waarden van de beide theta-matrices te verkrijgen. Retourneer deze beide waarden uit de methode <code>nn_check_gradients</code>.</p> <p>Wanneer je dit hebt gedaan, kun je het script verder laten runnen. Nu wordt van beide matrices de totale som afgedruk; die is nog behoorlijk hoog en het is de bedoeling dat we deze som, en de totale kosten, naar beneden brengen. Daar gaat de laatste opgave over.</p>"},{"location":"week4/opgave2-2.html#4-trainen-van-het-neurale-netwerk","title":"4. trainen van het neurale netwerk","text":"<p>Nu het netwerk goed is ge\u00efmplementeerd, kunnen we het gaan trainen. Het idee van die training is dat de waarden in de matrices langzaamaan naar een optimale waarde convergeren, waarmee de voorspelling voor een bepaalde input correspondeert met een goede output (een goede classificatie).</p> <p>Om dit gedaan te krijgen, maken we gebruik van de methode <code>minimize</code> uit <code>scipy.optimize</code>. Deze methode heeft een aantal parameters, waarvan \u00e9\u00e9n het maximaal aantal iteraties is. De methode stopt wanneer het maximaal aantal iteraties is behaald, of het minimale waarde van de kosten heeft gevonden (wat maar het eerst optreedt). Initieel staat deze parameter op 30. Let op: het trainen van het netwerk kan even duren (je krijgt wel het iteratienummer te zien).</p> <p>Je hoeft voor deze opgave niets uit te programmeren. Bestudeer de werking van <code>minimize</code> en experimenteer met verschillende waarden voor de parameter <code>maxiter</code> om een goed beeld te krijgen van de werking van het geheel. Bekijk ook goed hoe de waarden van de matrices worden doorgegeven aan deze methode.</p> <p>Als het geheel is afgerond, worden de nieuwe kost en de nieuwe accuratesse van het netwerk getoond. Ook wordt er een plot gemaakt van de waarden van de matrix in de verborgen laag \u2013 zie de figuur hieronder. Met een beetje moeite en goede wil kun je  zien dat deze matix een gevoeligheid heeft ontwikkeld voor horizontale en verticale lijnen en voor ronde vormen in de input.</p> <p></p> <p>Met de getrainde waarde van het netwerk kunnen we nu voorspellingen doen over het getal dat door een afbeelding wordt gerepresenteerd. Maar de accuratesse is niet de enige metriek die voor de bepaling van hoe goed een netwerk is van belang is. Hierover gaan we het volgende week hebben, wanneer we de confusion matrix bespreken.</p>"},{"location":"week5/index.html","title":"Stof: Model-evaluatie","text":"<ul> <li>De confusion matrix en de bijbehorende metrieken</li> <li>ROC / AUC</li> </ul>"},{"location":"week6/index.html","title":"Stof: Andere modellen","text":"<ul> <li>SVC</li> <li>k-means en DBSCAN</li> <li>beslisbomen</li> <li>Random Forest</li> </ul>"},{"location":"week6/inleveren.html","title":"Inleveren deel 3","text":"<ul> <li>Maak de opgavenset over neurale netwerken en de evaluatie daarvan</li> <li>Maak de opgavenset over modelevaluatie</li> <li>Opgaven SVC en DBSCAN: heeft Bart, nog vergaren</li> </ul>"},{"location":"week6/opgave3-1.html","title":"Deel 3 - opgaveset 1","text":""},{"location":"week6/opgave3-1.html#inleiding","title":"Inleiding","text":"<p>Deze week staat in het teken van TensorFlow en Keras. Tot nu toe hebben we de programmacode die bij de wiskunde hoorde zelf uitgeprogrammeerd, maar in de praktijk zul je dat niet heel vaak tegenkomen. Omdat de wiskunde behoorlijk complex kan worden, en feitelijk toch altijd min of meer hetzelfde is, zijn deze twee frameworks ontwikkeld om dergelijke implementatiedetails te abstraheren. Door deze (en vergelijkbare \u2013 er zijn er meer) frameworks te gebruiken, is het voor de ontwikkelaar mogelijk om zich te richten op de daadwerkelijke architectuur en optimalisatie-strategie\u00ebn.&lt;</p> <p>In de eerste opgave gaan we werken met een standaard-dataset, die in TensorFlow is ingebakken, om een netwerk te trainen. De tweede opgave gaat op basis van dit netwerk een confusion matrix uitrekenen en tekenen. In de derde en laatste opgave moet je een hele architectuur from scratch uitdenken en uitwerken.</p> <p>De startcode en andere bestanden die bij deze opgave horen kun je hier downloaden. Net als de vorige twee weken is er een bestand <code>exercise3.py</code>, dat het bestand <code>uitwerkingen.py</code> gebruikt. Het is de bedoeling dat je dit laatste bestand afmaakt.  </p>"},{"location":"week6/opgave3-1.html#opgave-1-de-fashion-mnist","title":"Opgave 1: de fashion MNIST","text":"<p>In de eerste week hebben we gewerkt met de MNIST dataset, die handschriften van een paar duizend scholieren bevatte. Deze week gebruiken we een andere, vergelijkbare dataset, namelijk de fashion-mnist. Deze set bevat afbeeldingen van mode-items, zoals broeken, jurken en schoenen. De set bevat 60.000 trainingsplaatjes en 10.000 testplaatjes. Elk plaatje is een 28\u00d728 grayscale plaatje.</p> <p>Het bestand <code>exercise3.py</code> begint met het inladen van een aantal dependencies en laadt vervolgens de dataset in. Deze zit standaard in TensorFlow \u2013 bestudeer het script om hier een beeld van te krijgen. Het laden van de data kan de eerste keer even duren (uiteraard afhankelijk van de snelheid van je downstream). Als de data geladen is, worden de dimensies van de verschillende data-sets uitgeprint.</p>"},{"location":"week6/opgave3-1.html#opgave-1a-het-visualiseren-en-prepareren-van-de-data","title":"opgave 1a: het visualiseren en prepareren van de data","text":"<p>Zoals altijd beginnen we met het visualiseren van de data. Maak de methode <code>plotImage()</code> in <code>uitwerkingen</code> af. Deze methode krijgt een array van 28\u00d728 als parameter mee en maakt gebruik van <code>pyplotlib</code> om hier een plaatje van te tekenen; ook wordt het label dat met het plaatje correspondeert aan de methode meegegeven. Zorg ervoor dat dit label onderaan het plaatje komt te staan. Het script <code>exercise3.py</code> roept deze methode aan met een willekeurige sample uit de dataset, zodat je eenvoudig kunt controleren of het plaatje correspondeert met het label.</p> <p></p> <p>Net als de vorige weken kun je het tekenen van het plaatje vervolgens overslaan door de parameter <code>skip</code> aan het script mee te geven.</p>"},{"location":"week6/opgave3-1.html#opgave-1b-aanpassen-van-data","title":"opgave 1b: aanpassen van data","text":"<p>Als je de data van de plaatjes bestudeert, zie je dat de getallen waaruit deze zijn opgemaakt liggen in de range van 0-255. Om deze goed door een neuraal netwerk te laten verwerken, is het van belang deze range om te zetten in getallen tussen de nul en de \u00e9\u00e9n. Implementeer de methode <code>scaleData()</code>, zodat de waarden van de getallen in de daaraan meegegeven matrix omgezet worden in de range 0-1. Zorg er daarbij voor, dat hier een willekeurige matrix aan kan worden meegegeven (dus ook \u00e9\u00e9n, waarbij de range van de oorsponkelijke waarden ligt tussen 0 en 1024). Maak gebruik van de numpy-methode amax om het hoogste getal in de meegegeven matrix te bepalen.</p> <p>Als je deze methode hebt ge\u00efmplementeerd, roept het script <code>exercise3.py</code> hem aan, zodat je kunt controleren of het klopt. Vervolgens wordt deze methode aangeroepen met <code>train_images</code> en <code>test_images</code>.</p>"},{"location":"week6/opgave3-1.html#opgave-1c-het-maken-van-het-model","title":"opgave 1c: Het maken van het model","text":"<p>Nu we de data hebben voorbereid, is het tijd om het model te maken. Tijdens de theorieles is ingegaan op de manier waarop je met Keras moet werken: dat moet je bij deze opgave toepassen. Maak in de methode <code>createModel</code> een netwerk met drie lagen: een input-laag die de plaatjes van 28\u00d728 omzet in 784 input-nodes; een tweede laag van 128 nodes die volledig verbonden is met de input-laag; en een derde laag met tien output-nodes. Geef aan de verborgen middelste laag een tf.nn.relu-activatie mee, en in de output-laag een tf.nn.softmax. </p> <p>Het kan zijn dat je wat <code>deprecation-warnings</code> krijgt bij het aanmaken van dit model (afhankelijk van de versie van TensorFlow die je gebruikt). Die kun je gevoegelijk negeren.</p> <p>Het model moet verder voorzien worden van een aantal parameters:</p> <ul> <li> <p>De optimizer, die aangeeft hoe het model wordt ge\u00fcpdate op basis van de data en de loss-function.</p> </li> <li> <p>De loss-function, die aangeeft hoe de accuratesse van het model gedurende de trainingsronden wordt bepaald. </p> </li> <li> <p>De metrics, waarmee de training en de tests worden gemonitord.</p> </li> </ul> <p>Geef deze parameters respectievelijk de waarden <code>sparse_categorical_crossentropy</code>, <code>adam</code>, en <code>accuracy</code>. Bestudeer de documentatie voor nadere specificaties hiervan. Retourneer het model vanuit de methode <code>buildModel()</code>.</p> <p>Wanneer deze opgave is afgerond, kun je het script <code>exercise3.py</code> opnieuw runnen. Hier wordt nu de methode <code>fit</code> op het model aangeroepen om het te trainen. In de volgende opgave gaan we vervolgens in op het bepalen van de kwaliteit van het getrainde netwerk.</p>"},{"location":"week6/opgave3-1.html#opgave-2-de-confusion-matrix","title":"Opgave 2 \u2013 de confusion matrix","text":"<p>In deze opgave gaan we op basis van de test-data bepalen hoe goed ons getrainde netwerk is. Zoals bekend is de accuratesse op zich niet voldoende om de presentatie van een classifier te bepalen: wanneer je gewoon zou gokken heb je bij tien mogelijke categorie\u00ebn al een score van tien procent, en als je zou gokken dat een sample iets niet is, is de accuratesse al negentig procent. Betere metrieken hiervoor worden gegeven door de confusion matrix, die tijdens de theorieles besproken is. </p> <p></p>"},{"location":"week6/opgave3-1.html#opgave-2a-bepalen-van-de-confusion-matrix","title":"Opgave 2a: bepalen van de confusion matrix","text":"<p>Maak de methode <code>confMatrix()</code> af. Zoals je ziet krijgt deze methode twee parameters mee, namelijk de door het netwerk voorspelde waarden, en de daadwerkelijke waarden. Maak gebruik van de methode <code>confusion_matrix</code> in TensorFlow om deze matrix te bepalen. Retourneer de matrix.</p> <p>Als je hiermee klaar bent, wordt de methode door het script <code>exercise3.py</code> aangeroepen, met de voorspellingen van de <code>test_images</code> en actuele waarden van die test-data (<code>test_labels</code>). Het resultaat wordt vervolgens in een plot weergegeven; als het goed is, ziet het er ongeveer als volgt uit: </p> <p></p>"},{"location":"week6/opgave3-1.html#opgave-2b-tp-tn-fp-fn","title":"Opgave 2b: TP, TN, FP, FN","text":"<p>Om meer metrieken uit het getrainde model te halen, volstaat niet alleen het percentage van de samples dat correct is geclassificeerd; we moeten dan ook weten welk percentage terecht als niet van een bepaalde klasse is geclassificeerd, welke onterecht als wel van een bepaalde klasse, en welk percentage onterecht als wel van een bepaalde klasse: de zogenaamde true positives, true negatives, false positives en false negatives (zoals in het theoriecollege besproken is).</p> <p>De methode <code>confEls()</code> in <code>uitwerkingen.py</code> krijgt als parameter een numpy-array mee, die ook ten grondslag ligt aan de afbeelding hierboven. De regels van deze matrix corresponderen met de werkelijke waarde van het sample, de kolommen van deze matrix corresponderen met de voorspelling van het sample door het model. Hoewel er semantisch wel het \u00e9\u00e9n en ander op aan te merken is, defini\u00ebren we de hierboven beschreven metrieken als volgt:</p> <ul> <li> <p>\\(tp_{i} = c_{ii}\\)</p> </li> <li> <p>\\(fp_{i} = \\sum_{l=1}^n c_{li} - tp_{i}\\)</p> </li> <li> <p>\\(fn_{i} = \\sum_{l=1}^n c_{il} - tp_{i}\\)</p> </li> <li> <p>\\(tn_{i} = \\sum_{l=1}^n \\sum_{k=1}^n c_{lk} -tp_{i} - fp_{i} - fn_{i}\\)</p> </li> </ul> <p>Hierbij is \\(i\\) de categorie in kwestie (dus in dit specifieke geval loopt die van 1 - 10). </p> <p>Implementeer de methode <code>confEls()</code>, en retourneer een lijst met deze vier metrieken voor elk label \u2013 bestudeer het reeds gegeven deel van de implementatie om een beeld te krijgen van de exacte vorm van de return-waarde.</p>"},{"location":"week6/opgave3-1.html#opgave-2c-precision-en-recall","title":"Opgave 2c: precision en recall","text":"<p>Implementeer nu de methode <code>confData()</code>, waarin je de data die je in de vorige opgave hebt gemaakt omzet in de onderstaande metrieken: </p> <ul> <li> <p>\\(sensitivity (TPR) = \\frac{tp}{tp + fn}\\)</p> </li> <li> <p>\\(precision (PPV) = \\frac{tp}{tp + fp}\\)</p> </li> <li> <p>\\(specificity (TNR) = \\frac{tn}{tn + fp}\\)</p> </li> <li> <p>\\(fall-out (FPR) = \\frac{fp}{fp + tn}\\)</p> </li> </ul> <p>Deze methode krijgt de lijst uit de vorige opgave mee: de totale \\(tp\\) is dan de som van alle \\(tp\\)'s van alle labels \u2013 en vergelijkbare berekeningen voor de total \\(tn\\), \\(fp\\) en \\(fn\\). Retourneer deze data als een dictionary. Als je deze beide methoden hebt ge\u00efmplementeerd, kun je het script <code>exercise3.py</code> nogmaals runnen; Hierdoor worden deze waarden afgedrukt. Zeg op basis van deze resultaten iets over de kwaliteit van het uitgeprogrammeerde netwerk.</p>"},{"location":"week7/index.html","title":"Stof: Hyperparameter tuning","text":"<ul> <li>hyperparameter tuning</li> <li>GridSearchCV</li> <li>RandomizedSearchCV</li> <li>...and their halving counterparts</li> </ul>"},{"location":"week8/index.html","title":"Stof: geavanceerde onderwerpen","text":"<ul> <li>large language models</li> <li>dimensionaliteitsreductie en PCA</li> <li>ensemble learning</li> <li>informatie-entropie</li> </ul>"},{"location":"week8/inleveren.html","title":"Inleveren deel 4","text":"<ul> <li>Opgave over hyperparam tuning: nog ontwikkelen</li> <li>Vrije opgave over leuke dingen: nog ontwikkelen</li> </ul>"}]}