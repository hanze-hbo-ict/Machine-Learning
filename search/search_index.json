{"config":{"lang":["nl"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welkom bij Machine Learning","text":"<p>Hier vindt u onder andere de stof (per week) en de opgaven (per blok, of \"deel\", van twee weken).</p>"},{"location":"index.html#inleiding","title":"Inleiding","text":"<p>Machine learning is het onderdeel van de informatica dat zich bezighoudt met het analyseren van grote hoeveelheden data en op basis daarvan structuren herkennen of voorspellingen doen. Hoewel de basis voor dit vakgebied al in de jaren zestig van de vorige eeuw is gelegd (bijvoorbeeld door het baanbrekende werk van Frank Rosenblatt, is het pas de voorbije twee decennia echt tot grote bloei gekomen. Dit heeft vooral te maken met de enorme hoeveelheid data die heden ten dage beschikbaar wordt gesteld (een fenomeen bekend onder de term information explosion in combinatie met de grote en goedkope computationele kracht van hedendaagse computers en data-centra.</p>"},{"location":"index.html#opgaven","title":"Opgaven","text":"<p>Op deze pagina's zijn de weekopgaven van dit onderdeel te vinden (zie het menu links).</p> <p>De practicumopdrachten worden bij voorkeur gemaakt in duo's. Tijdens het eerste practicum wordt klassikaal een begin gemaakt met de opgaven: er wordt uitgelegd hoe te werken met NumPy en hoe je lineaire algebra gebruikt in het domein van ML. Ook worden hier de weekopgaven verder toegelicht.</p> <p>Tijdens het tweede, vierde, zesde en achtste practicum worden (tijdens de geroosterde oplevermomenten) afspraken gemaakt met de duo's om het werk tot dan toe te bespreken en te beoordelen.</p> <p>Er zijn vier sets weekopgaven die uiteindelijk allemaal in de voorlaatste week van het blok moeten zijn afgerond. Elke set moet met een voldoende cijfer beoordeeld zijn. Eventueel reparatiewerk wordt tijdens de oplevermomenten besproken. Mocht dat niet voor de deadline zijn afgerond, dan is er een uitloopmogelijkheid in de laatste week van het blok. Dat geldt dan wel als een herkansing. Een niet-ingeleverde opdracht krijgt het cijfer 1.</p>"},{"location":"index.html#opstarten","title":"Opstarten","text":"<p>De opgaven voor elke week gaan uit van een zipje met startcode. Het geheel gaat uit van een aantal dependencies. Deze dependencies hebben we voor het gemak in een <code>requirements.txt</code> gezet. Je kunt het beste een virtuele omgeving aanmaken en hierin met pip in \u00e9\u00e9n keer alle dependencies installeren. Hier een voorbeeld voor MacOS:</p> <pre><code>baba@aurelia ~ % python -m venv ml\nbaba@aurelia ~ % cd ml \nbaba@aurelia ml % cp ~/Downloads/requirements.txt .\nbaba@aurelia ml % source bin/activate\n(ml) baba@aurelia ml % python -m pip install -r requirements.txt \n...\n(ml) baba@aurelia ml % \n</code></pre> <p>en voor Windows (Terminal/PowerShell) <pre><code>PS C:\\Users\\ez&gt; python -m venv ml\nPS C:\\Users\\ez&gt; cd ml\nPS C:\\Users\\ez\\ml&gt; cp ~/Downloads/requirements.txt .\nPS C:\\Users\\ez\\ml&gt; Scripts/activate\n(ml) PS C:\\Users\\ez\\ml&gt; python -m pip install -r requirements.txt\n...\n(ml) PS C:\\Users\\ez\\ml&gt; \n</code></pre></p>"},{"location":"index.html#globale-planning","title":"Globale planning","text":"Weeknummer Onderwerp Inleveren 1 Tools en technieken 2 Lineaire regressie opgaven deel 1 3 Logistische regressie 4 Neurale netwerken opgaven deel 2 5 Model-evaluatie en Hyperparameter tuning 6 Andere modellen en Ensemble learning opgaven deel 3 7 Dimensionaliteitsreductie en (grote) taalmodellen 8 Recurrente neural netwerken opgaven deel 4"},{"location":"index.html#stof-en-opgaven-per-week","title":"Stof en opgaven per week","text":""},{"location":"index.html#deel-1","title":"Deel 1","text":"<ul> <li>Week 1</li> <li>Week 2</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-2","title":"Deel 2","text":"<ul> <li>Week 3</li> <li>Week 4</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-3","title":"Deel 3","text":"<ul> <li>Week 5</li> <li>Week 6</li> <li>Inlevermoment</li> </ul>"},{"location":"index.html#deel-4","title":"Deel 4","text":"<ul> <li>Week 7</li> <li>Week 8</li> <li>Inlevermoment</li> </ul>"},{"location":"deel1/inleveren.html","title":"Inleveren deel 1","text":"<ul> <li>Maak de Notebook over Scikit Learn en de California Housing-dataset</li> <li>Maak de opgavenset over lineaire regressie</li> </ul>"},{"location":"deel1/week1.html","title":"Week 1: Tools en technieken","text":""},{"location":"deel1/week1.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Machine Learning: achtergrond, geschiedenis, termen en technieken</li> <li>Jupyter Noteboook</li> <li>Handige packages: Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn</li> <li>Hele cyclus doorlopen aan de hand van de Titanic-dataset: data opschonen, prepareren, model fitten, evalueren</li> </ul>"},{"location":"deel1/week1.html#college-sheets","title":"College-sheets","text":"<p>Na afloop van het hoorcollege komen hier de gebruikte slides en notebooks beschikbaar.</p>"},{"location":"deel1/week1.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 1 t/m 4 (ter voorbereiding)</li> <li>G\u00e9ron: hoofdstuk 1 en 2 (optioneel)</li> </ul>"},{"location":"deel1/week2.html","title":"Week 2: Lineaire regressie","text":""},{"location":"deel1/week2.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Lineaire regressie</li> <li>De kostenfunctie en stapsgewijze minimalisatie daarvan middels gradient descent</li> </ul>"},{"location":"deel1/week2.html#college-sheets","title":"College-sheets","text":"<p>Na afloop van het hoorcollege komen hier de gebruikte slides en notebooks beschikbaar.</p>"},{"location":"deel1/week2.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 5 en 6</li> <li>G\u00e9ron: hoofdstuk 2 en 4 (t/m Gradient Descent) (optioneel)</li> </ul>"},{"location":"deel1/opgaven/opgave1-2.html","title":"Deel 1 - opgaveset 2","text":""},{"location":"deel1/opgaven/opgave1-2.html#inleiding","title":"Inleiding","text":"<p>In deze opgave werken we aan de lineaire regressie van data met \u00e9\u00e9n variabele. De data die we daarvoor gebruiken is de winst van een vervoerder gerelateerd aan de grootte van een stad waar die vervoerder werkzaam is. Je kunt je voorstellen dat het nuttig is om deze verhouding te weten, omdat je op basis hiervan ge\u00efnformeerd een besluit kunt nemen of je in een nieuwe stad (met een bepaalde grootte) een nieuw filiaal wilt openen.</p> <p>De startcode van deze opgave kun je hier downloaden. Het bestand <code>week1_data.pkl</code> bevat de data waar we mee gaan werken. Dit is een 97\u00d72-numpy-vector: de eerste kolom bevat de grootte van de steden, de tweede kolom bevat de winst van de vervoerder.</p> <p>Daarnaast vind je twee python-bestanden in de zip. Het bestand <code>exercise1.py</code> gebruikt de code in <code>uitwerkingen.py</code> om door de opgaven te lopen. Het is de bedoeling dat je het bestand <code>uitwerkingen.py</code> afmaakt. Bestudeer de code en het commentaar in beide bestanden om te zien hoe ze werken en wat de bedoeling ervan is.</p>"},{"location":"deel1/opgaven/opgave1-2.html#1-het-visualiseren-van-de-data","title":"1. het visualiseren van de data","text":"<p>Een eerste stap in elk machine-learning project is een beeld cre\u00ebren van de data. Het eenvoudigst om dit te doen is door gebruik te maken van de library <code>matplotlib</code>. Hierin vind je een API <code>pyplot</code> waarmee je vrij eenvoudig een scatterplot kunt maken.</p> <p>Bestudeer de documentatie van <code>pyplot</code> en implementeer aan de hand hiervan de functie <code>draw_graph</code> in <code>uitwerkingen.py</code>, zodat je een afbeelding zoals de onderstaande krijgt. Hoewel de data niet echt normaal verdeeld is, is er wel een zekere verhouding waar te nemen tussen de grootte van de stad en de winst die de vervoerder maakt. In de rest van deze opgave gaan we deze verhouding bepalen.</p> <p></p>"},{"location":"deel1/opgaven/opgave1-2.html#2-het-bepalen-van-de-cost-function-kostenfunctie","title":"2. Het bepalen van de cost function (kostenfunctie)","text":"<p>Zoals in de theorieles besproken is, is het bepalen van de verhouding tussen twee (of meer) grootheden een kwestie van het minimaliseren van de kostenfunctie: de som van de gekwadrateerde fouten (SSE). Dit minimaliseren doe je door middel van gradient descent en het is dikwijls nuttig om die kostenfunctie gedurende de iteraties bij te houden, zodat je de verschillende uitkomsten door je data heen kunt plotten \u2013 zo kun je bijvoorbeeld controleren of je niet in een lokaal minimum terecht bent gekomen. In deze opgave programmeren we de kostenfunctie verder uit; de volgende opgave gaat verder in op de gradient descent.</p> <p>De kostenfunctie is gegeven door de volgende formule:</p> \\[  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \\] <p>Hierin is \\(J(\\theta)\\) de totale kost die berekend wordt met de huidige waarden van \\(\\theta\\). Verder is \\(h_{\\theta}(x)\\) de hypothese van de waarde (de voorspelling) en is \\(y\\) de actuele (daadwerkelijke) waarde. Door het verschil tussen deze twee waarden voor elk datapunt op te tellen en uiteindelijk uit te middelen, komen we op de voorspellende waarde die de formule heeft met de huidige waarden van \\(\\theta\\).</p> <p>De algemene formule voor de hypothese (de voorspelling) is als volgt: </p> \\[ h_\\theta(x) = \\theta^T\\vec{x} = \\theta_0 + \\theta_1x  \\] <p>Omdat we op zoek zijn naar een lijn, hebben we feitelijk te maken met \u00e9\u00e9n parameter (een lijn is immers \\(y=ax + b = b + ax\\)). Allereerst bepalen we het aantal datapunten (\\(m\\)) en het aantal eigenschappen (features, \\(n\\)). Om de dimensionaliteit van de trainingsdata te laten corresponderen met \\(\\theta\\), voegen we vervolgens een rij van enen toe. Daarna isoleren we de laatste kolom van de data om de gewenste waarden te krijgen (de vector \\(y\\)); de rest van de data vormt dan de eigenlijke matrix \\(X\\). Tenslotte initi\u00ebren we de vector \\(\\theta\\):</p> <pre><code>m,n = data.shape\nX = np.c_[np.ones(m), data[:, [0]]]\ny = data[:, [1]]\ntheta = np.zeros( (2, 1) )\n</code></pre> <p>De voorspelde waarden in \\(X\\), de actuele waarden in \\(y\\) en een \\(theta\\) worden aan de methode <code>compute_cost</code> meegegeven; deze functie moet de waarde van \\(J(\\theta)\\) teruggeven. Implementeer deze functie op basis van de beschrijving hierboven in het bestand <code>uitwerkingen.py</code> (zie ook de aanwijzingen in het bestand zelf); maak hem zo, dat hij werkt voor elke grootte van <code>theta</code>. Maak gebruik van een vectori\u00eble implementatie.</p> <p>Als je klaar bent, kun je het bestand <code>opgaven.py</code> runnen. Dit bestand roept <code>compute_cost</code> aan en print de gevonden waarde. Als het goed is, krijg je uiteindelijk een waarde van rond de 32.07.</p>"},{"location":"deel1/opgaven/opgave1-2.html#3a-gradient-descent","title":"3a. Gradient descent","text":"<p>Als laatste maak je de methode <code>gradient_descent</code> in het bestand <code>uitwerkingen.py</code> af. In deze methode wordt een aantal stappen uitgevoerd, waarbij in elke stap de vector \\(\\theta\\) volgens de onderstaande formule wordt aangepast.</p> \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\] <p>Als het goed is, zorgt elke stap in deze methode ervoor dat \\(J(\\theta)\\) lager wordt. Let er wel op dat je alle \\(\\theta_j\\) tegelijkertijd aanpast (in dit geval is de grootte van \\(\\theta\\) 2, dus elke iteratie moeten er twee parameters worden aangepast). Let er verder op dat je alleen \\(\\theta\\) aanpast: X en y zijn constante waarden die niet hoeven (of kunnen) te worden aangepast.</p> <p>De algemene structuur in het bestand is al gegeven. Als je implementatie klaar is, kun je opnieuw het bestand <code>opgaven.py</code> aanroepen; deze roept de functie <code>gradient_descent</code> aan, zodat de update 1500 keer wordt gedaan. Als het goed is, is \\(\\theta\\) uiteindelijk rond de (-3.63, 1.16).</p>"},{"location":"deel1/opgaven/opgave1-2.html#3b-kosten-bijhouden","title":"3b. kosten bijhouden","text":"<p>Omdat we, zoals gezegd, graag willen weten of de totale kost tijdens de gradient descent wel steeds minder wordt, is het van belang deze kosten tijdens deze descent bij te houden. Breid de functie <code>gradient_descent</code> aan zodat bij elke iteratie de functie <code>compute_cost</code> wordt aangeroepen met de huidige waarden van <code>theta</code>. Deze kosten stop je in de lijst <code>costs</code> (die al in de basiscode is ge\u00efnitialiseerd) en die retourneer je uiteindelijk eveneens (<code>gradient_descent</code> retourneert dus twee waarden).</p> <p>Maak vervolgens de functie <code>draw_costs</code> in <code>uitwerkingen.py</code>. Deze functie moet de lijst meekrijgen die je hierboven in <code>gradient_descent</code> hebt gevuld. Maak gebruik van <code>pyplot</code> om deze lijst in een grafiekje te zetten. Als je dan het bestand <code>exercise.py</code> runt, krijg je als het goed is iets als het onderstaande plaatje:</p> <p></p>"},{"location":"deel1/opgaven/opgave1-2.html#4-contourplot","title":"4. Contourplot","text":"<p>In deze laatste opgave gebruik je de methode <code>compute_cost</code> die je hierboven hebt gemaakt om een contour-plot van de kosten te tekenen. Hierdoor kun je inzicht krijgen in hoe deze waarde zich ontwikkelt bij verschillende waarden van \\(\\theta\\). </p> <p>Het grootste deel van deze opgave is al in de methode <code>contour_plot()</code> in het bestand <code>uitwerkingen.py</code> gegeven; je hoeft alleen maar de waarden van de matrix <code>J_val</code> te vullen. Bestudeer het commentaar in het bestand voor meer toelichting. Als je klaar bent, roept het bestand <code>exercise1.py</code> de methode <code>contour_plot()</code> aan om de plot te tekenen. Als het goed is, ziet deze er ongeveer als hieronder uit.</p> <p></p>"},{"location":"deel2/inleveren.html","title":"Inleveren deel 2","text":"<ul> <li>Maak de opgave over binaire classificatie</li> <li>Maak de opgavenset over neurale netwerken</li> <li>Maak de opgavenset over neurale netwerken met TensorFlow en Keras</li> </ul>"},{"location":"deel2/week3.html","title":"Week 3: Logistische regressie","text":""},{"location":"deel2/week3.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Classicatie-problemen</li> <li>Binaire en multi-class classificatie</li> <li>Logistische regressie</li> <li>De kostenfunctie van logistische regressie en stapsgewijze minimalisatie daarvan middels gradient descent</li> </ul>"},{"location":"deel2/week3.html#college-sheets","title":"College-sheets","text":"<p>Na afloop van het hoorcollege komen hier de gebruikte slides en notebooks beschikbaar.</p>"},{"location":"deel2/week3.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 7</li> <li>G\u00e9ron: hoofdstuk 3 en 4 (Logistic Regression) (optioneel)</li> </ul>"},{"location":"deel2/week4.html","title":"Week 4: Neurale netwerken","text":""},{"location":"deel2/week4.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Neurale netwerken / Artificial neural networks (ANNs)</li> <li>Forward propagation</li> <li>Backpropagation</li> <li>Tensorflow en Keras</li> </ul>"},{"location":"deel2/week4.html#college-sheets","title":"College-sheets","text":"<p>Na afloop van het hoorcollege komen hier de gebruikte slides en notebooks beschikbaar.</p>"},{"location":"deel2/week4.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 8</li> <li>G\u00e9ron: hoofdstuk 10</li> </ul>"},{"location":"deel2/opgaven/opgave2-1.html","title":"Deel 2 - opgaveset 1","text":""},{"location":"deel2/opgaven/opgave2-1.html#inleiding","title":"Inleiding","text":"<p>We gaan deze week aan de slag met binaire classificatie: behoort een element wel (1) of niet (0) tot een verzameling? We werken met de standaard Iris-dataset van sklearn. We gaan proberen te voorspellen of bloemen wel of niet van de ondersoort Iris Virginica zijn. Hiervoor gebruiken we logistische regressie, de bijbehorende kostenfunctie en de stapsgewijze aanpassing daarvan middels gradient descent.</p>"},{"location":"deel2/opgaven/opgave2-1.html#stappenplan","title":"Stappenplan","text":"<ul> <li>Download de dataset met behulp van <code>load_iris()</code> uit <code>sklearn.datasets</code>.</li> <li>Vul je featurematrix <code>X</code> op basis van de data.</li> <li>De uitkomstvector <code>y</code> ga je vullen op basis van target. Standaard bevat deze array de waardes 0, 1 en 2 (resp. 'setosa', 'versicolor', 'virginica'). Maak deze binair door 0 en 1 allebei 0 te maken (niet-virginica) en van elke 2 een 1 te maken (wel-virginica). Denk erom dat y het juiste datatype en de juiste shape krijgt.</li> <li>Definieer een functie <code>sigmoid()</code> die de sigmo\u00efde-functie implementeert.</li> <li>Initialiseer een vector theta met 1.0'en in de juiste shape.</li> <li>Nu kun je beginnen aan de loop waarin je in 1500 iteraties:<ul> <li>De voorspellingen (denk aan sigmoid!) en de errors berekent.</li> <li>De gradient berekent en theta aanpast. Werk in eerste instantie met een learning rate van 0.01.</li> <li>De kosten berekent.</li> </ul> </li> <li>Als het goed is, zie je de kosten (vanaf een beginwaarde rond 8) steeds dalen kom je aan het einde rond 0,24 uit. Werk je met de niet-negatieve versie van de kostenfunctie, dan ga ja van ongeveer -8 naar -0,24.</li> <li>Experimenteer eens met andere waardes van de learning rate (1.0 &lt; alpha &lt; 0.0) en het aantal iteraties.</li> </ul> <p>Tijdens het inlevermoment demonstreer je je notebook, licht je een en ander toe en beantwoord je vragen hierover.</p>"},{"location":"deel2/opgaven/opgave2-2.html","title":"Deel 2 - opgaveset 2","text":""},{"location":"deel2/opgaven/opgave2-2.html#inleiding","title":"Inleiding","text":"<p>In deze opgave werken we met een standaard dataset, de zogenaamde MNIST-dataset: een set van zevenduizend grayscale afbeeldingen van cijfers en letters geschreven door middelbare scholieren. Werken met de MNIST-dataset is de hello world van machine learning: vroeg of laat krijg je ermee te maken. In deze opgave programmeren we zelf een neuraal netwerk aan de hand van reeds geleerde gewichten; in de volgende set zullen we een framework gebruiken in een poging een andere dataset te classificeren.</p> <p>De set die we in deze opgave gebruiken is een subset van de oorspronkelijke dataset. Het gaat om vijfduizend samples, waarbij elk sample een plaatje van 20 bij 20 pixels is, dat een getal van 0 tot 9 representeert. Elke kolom van deze 20 \u00d7 20 matrix is onder de vorige geplakt, zodat er uiteindelijke een 400 \u00d71 vector ontstaat. Deze vectoren zijn weer getransponeerd, zodat onze dataset \\(X\\) uiteindelijk een 5000 \u00d7 400 matrix is.</p> \\[ X = \\begin{bmatrix} &amp; \u2014 &amp; (x^{(1)})^T &amp; \u2014 &amp; \\\\ &amp; \u2014 &amp; (x^{(2)})^T &amp; \u2014 &amp; \\\\ &amp; &amp; \\vdots &amp; &amp; \\\\ &amp; \u2014 &amp; (x^{(m)})^T &amp; \u2014 &amp; \\\\ \\end{bmatrix} \\] <p>In deze matrix is \\(x^{(1)}\\) de vector van het eerste plaatje, \\(x^{(2)}\\) de vector van het tweede plaatje, enzovoort.  Behalve deze X-matrix is er in de data ook een 5000\u00d71 vector y gegeven, waarin per plaatje is aangegeven welk cijfer dit representeert.</p> <p>De startcode voor deze opgave is hier te downloaden. Net als in de vorige set wordt deze opgave doorlopen door het script <code>exercise2.py</code>. Dit script importeert de functies uit <code>uitwerkingen.py</code> en runt die op volgorde. Het is de opgave om deze uitwerkingen af te maken. Bestudeer beide scripts om een idee te krijgen van de werking.</p>"},{"location":"deel2/opgaven/opgave2-2.html#1-het-visualiseren-van-de-data","title":"1. het visualiseren van de data","text":"<p>Zoals altijd gaan we eerst de data visualiseren. In dit geval betekent dat een vector uit \\(x\\) weer transformeren in een 20\u00d720 grayscale plaatje. Maak hiervoor de functie <code>plot_number()</code> af. Omdat je weet dat de vector \\(x^{(i)}\\) het i-de plaatje uit de dataset representeert, kun je deze vector eenvoudig weer terug omzetten in een 20\u00d720 matrix en die tekenen. Maak daarbij gebruik van de methode <code>numpy.reshape</code> en van <code>matplotlib.matshow</code>. Het script roept de methode <code>plot_number</code> aan met een willekeurige vector uit de matrix X. Het toont het nummer op de console, en het cijfer dat het plaatje zou moeten voorstellen. Op die manier kun je eenvoudig checken of je uitwerking correct is.</p> <p></p> <p>Als je deze opgave hebt afgerond, kun je het script aanroepen met <code>skip</code> als commandline parameter: het tekenen wordt dan overgeslagen.</p>"},{"location":"deel2/opgaven/opgave2-2.html#2-het-neurale-netwerk-forward-propagation","title":"2. het neurale netwerk - forward propagation","text":"<p>Het neurale netwerk dat we voor deze opgave gaan uitprogrammeren bestaat uit drie lagen. De input van het netwerk zijn de 20\u00d720 plaatjes van de handgeschreven cijfers, dus de input-laag bestaat uit 400 nodes. De middelste verborgen laag heeft 25 nodes en de output-laag heeft tien nodes \u2013 \u00e9\u00e9n voor elk cijfer van nul tot en met negen. Gegeven een bepaalde input moet \u00e9\u00e9n van de tien outputnodes de hoogste waarde hebben.</p> <p></p>"},{"location":"deel2/opgaven/opgave2-2.html#2a-de-sigmoid-functie","title":"2a: de sigmoid functie","text":"<p>Het bepalen van het cijfer dat door het plaatje wordt gerepresenteerd is feitelijk een classificatieprobleem: de input vector \\(x^{(i)}\\) moet immers geclassificeerd worden als \u00e9\u00e9n van de cijfers 0 tot en met 9. Zoals tijdens de theorieles is besproken, hebben we voor dergelijke problemen de sigmo\u00efde-functie \\(g(z)\\) nodig. De formule daarvan is als volgt:</p> \\[ g(z) = \\frac{1}{1+e^{-z}} \\] <p>Implementeer de methode <code>sigmoid</code> in <code>uitwerkingen.py</code>. Maak hem zo, dat je er zowel een getal als een vector aan kunt meegeven. In het eerste geval moet de functie de sigmo\u00efde-waarde van het getal retourneren, in het tweede geval moet hij een vector retourneren met de sigmo\u00efde-waarde van elk individueel element in de inputvector. Je kunt gebruik maken van de numpy-functie <code>exp()</code>.</p> <p>Als je klaar bent, kun je het script <code>exercise2.py</code> runnen. Deze roept de methode vijf keer aan: drie keer met individuele getallen -10, 0, en 10; vervolgens met deze drie getallen als een kolomvector en als een rijvector.</p>"},{"location":"deel2/opgaven/opgave2-2.html#2b-omzetten-van-een-vector-naar-een-matrix","title":"2b: omzetten van een vector naar een matrix","text":"<p>We beginnen met het omzetten van de rijvector \\(y\\) naar een matrix. Voor het berekenen van de kost van het netwerk moeten we namelijk deze vector omzetten in een 5000 \u00d7 10 matrix van enen en nullen, waarbij het i-de element 1 is en de rest 0. Als bijvoorbeeld het label in \\(y\\) gelijk is aan 5, dan moeten we deze rij omzetten in een 10-dimensionale vector met een 1 op positie 5 en een 0 op de rest.</p> \\[ y=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, ... , of \\begin{bmatrix}0\\\\0\\\\0\\\\\\vdots\\\\1\\end{bmatrix} \\] <p>Maak de methode <code>get_y_matrix()</code> in <code>uitwerking.py</code> af. Hierbij kun je gebruik maken van de methode csr_matrix uit scipy om op basis van y de matrix <code>y_vec</code> te maken. Let er daarbij wel op dat de y-vector 1-based is, terwijl de resulterende matrix 0-based moet zijn. Zie het onderstaande code-fragment voor een voorbeeld (je kunt ook gebruik maken van de methode todense() om een zogenaamde ijle matrix te maken):</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.sparse import csr_matrix\n&gt;&gt;&gt; cols = np.array([ 2,1,3,5 ])\n&gt;&gt;&gt; rows = [i for i in range(len(cols))]\n&gt;&gt;&gt; data = [1 for _ in range(len(cols))]\n&gt;&gt;&gt; width = max(cols) + 1 # arrays zijn zero-based\n&gt;&gt;&gt; y_vec = csr_matrix((data, (rows, cols)), shape=(len(rows), width+1)).toarray()\n&gt;&gt;&gt; y_vec\narray([[0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0]])\n&gt;&gt;&gt; \n</code></pre> <p>Als je klaar bent, kun je het script <code>exercise2.py</code> opnieuw runnen om de geretourneerde waarde te controleren.</p>"},{"location":"deel2/opgaven/opgave2-2.html#2c-voorspel-het-getal-en-bepaal-de-kost-van-deze-voorspelling","title":"2c. voorspel het getal en bepaal de kost van deze voorspelling.","text":"<p>Implementeer nu de methode <code>predict_number</code>. Maak hierbij gebruik van het stappenplan dat is gegeven in de afbeelding van het netwerk hierboven, en van het commentaar in de opgave. Let er bij je implementatie op dat de matrix <code>X</code> de waarden voor de input-nodes als rij bevat (400 rijen), terwijl de matrices \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) de waarde voor elke node als kolom bevatten (zo is \\(\\Theta^{(1)}_{2,3}\\) het gewicht tussen de derde input-node en de tweede verborgen node). Voorzie de data van de juiste indices en transpositioneer de matrix waar dat nodig is. De output van deze methode is een 5000 \u00d7 10 vector die voor elk getal de waarschijnlijkheid dat de input dat getal is weergeeft.</p> <p>Om de accuratesse van deze voorspelling te bepalen, moeten we deze voorspelling vergelijken met de betreffende regel uit de y_matrix die we in het eerste deel van deze opgave hebben gemaakt. Dat gebeurt in de methode <code>compute_cost()</code>. De formule voor de kost is als volgt:</p> \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k\\right] \\] <p>Als je deze drie methoden hebt ge\u00efmplementeerd, kun je het script <code>exercise2.py</code> opnieuw aanroepen. Hier wordt begonnen met min of meer willekeurige waarden van de Theta's. We zetten deze niet op 0 (waarom niet?), maar verdelen we uniform in de range \\([-0.12, 0.12]\\) \u2013 zie hiervoor de methode <code>initialize_random_weights</code> in het script. Deze waarde is gebaseerd op het aantal nodes in het netwerk en wordt benaderd door </p> \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L{out}}} \\] <p>waarbij \\(L_{in}\\) en \\(L_{out}\\) staan voor het aantal nodes links en rechts van de betreffende laag. Het script voert met deze waarden van de matrices een forward propagation stap uit en toont de kost die correspondeert met de huidige waarden van de beide <code>Theta</code>'s. Als het goed is, ligt dit zo rond de 7 (de exact waarde is natuurlijk moeilijk te voorspellen). Vervolgens wordt de huidige accuratesse van het netwerk getoond (die is vanzelfsprekend extreem laag).</p>"},{"location":"deel2/opgaven/opgave2-2.html#3-het-neurale-netwerk-backpropagation","title":"3. het neurale netwerk \u2013 backpropagation","text":""},{"location":"deel2/opgaven/opgave2-2.html#3a-de-sigmoidegradient","title":"3a. de sigmo\u00efdegradi\u00ebnt","text":"<p>Om de relatieve bijdrage van een node te bepalen, hebben we de afgeleide van de sigmo\u00efdefunctie nodig. Deze is hieronder gegeven.</p> \\[ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) \\] <p>Implementeer deze afgeleide in de methode <code>sigmoid_gradient</code>. Zorg er opnieuw (net als bij de sigmo\u00efde zelf) voor dat deze methode zowel met scalaire waarden als met vectoren kan werken. Om zeker te weten of het goed is gegaan, roept het script <code>exercise.py</code> deze methode weer aan met drie verschillende waarden.</p>"},{"location":"deel2/opgaven/opgave2-2.html#3b-backpropagation","title":"3b. backpropagation","text":"<p>We gaan nu de backpropagation uitprogrammeren in de methode <code>nn_check_gradients</code>. De intu\u00eftie hierbij is als volgt: voor een observatie \\((x^{(i)}, y^{(i)})\\) doen we eerst een forward-propagation stap, waarbij we de voorspelling \\(a_3 = h_\\Theta(x^{(i)})\\) uitrekenen. Als we die hebben bepaald, willen we voor elke node in elke laag in het netwerk bepalen wat de bijdrage van deze node aan de totale fout is geweest. Deze fout geven we weer met \\(\\delta_j^{(l)}\\), waarbij \\(l\\) de laag en \\(j\\) het nummer van de betreffende node is.</p> <p>Voor de output-laag is de fout redelijk rechttoe-rechtaan te bepalen: dit is per node het verschil tussen diens output en de gewenste output (zoals opgeslagen in de matrix <code>y_vec</code>). </p> <p>Voor de verborgen nodes is dit iets complexer: de fout in laag \\(l\\) wordt berekend aan de hand van de gemiddelde fout in de laag \\(l+1\\). Zie het stappenplan hieronder; we adviseren je om de backpropagation uit te programmeren in een for-lus (<code>for i in range (m):</code>), omdat een vectori\u00eble implementatie complex is en een na\u00efeve implementatie waarschijnlijk leerzamer.</p> <p>Stap 1</p> <p>Gegeven een input \\(a^{(i)}\\), doe een standaard forward-propagation door gebruik te maken van de code uit <code>predict_number</code>; je moet de code hier wel herhalen, omdat je de verschillende waarden nodig hebt tijdens de backpropagation.</p> <p>Stap 2</p> <p>Zet voor elke output-node <code>k</code> in de derde laag </p> \\[ \\delta_k^{(3)} = (a_k^{(3)} - y_k) \\] <p>Stap 3</p> <p>Voor elke node in de verborgen (tweede) laag bepaal je diens 'bijdrage' aan de totale fout door het inproduct van de fout en de matrix (element wise) te vermenigvuldigen met de afgeleide van de sigmo\u00efdefunctie (die we in het eerste deel van deze opgave hebben gemaakt).</p> \\[ \\delta^{(2)} = (\\Theta^{(2)})^T\\cdot\\delta^{(3)} \\times g'(z^{(2)}) \\] <p>Stap 4</p> <p>Deze bijdrage tellen we op bij de andere bijdragen; op deze manier cre\u00ebren we twee nieuwe matrices \\(\\Delta^{(1)}\\) en \\(\\Delta^{(2)}\\), die dezelfde dimensionaliteit hebben als \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\). (De variable l staat voor het nummer van de laag en is in dit geval 1 of 2.)</p> \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}\\cdot(a^{(l)})^T \\] <p>Stap 5</p> <p>Als we deze stappen voor alle \\(m\\) observaties hebben gedaan, delen we de beide delta-matrices door het aantal observaties (element wise) om de gemiddelde fout per node voor de huidige waarden van de beide theta-matrices te verkrijgen. Retourneer deze beide waarden uit de methode <code>nn_check_gradients</code>.</p> <p>Wanneer je dit hebt gedaan, kun je het script verder laten runnen. Nu wordt van beide matrices de totale som afgedruk; die is nog behoorlijk hoog en het is de bedoeling dat we deze som, en de totale kosten, naar beneden brengen. Daar gaat de laatste opgave over.</p>"},{"location":"deel2/opgaven/opgave2-2.html#4-trainen-van-het-neurale-netwerk","title":"4. trainen van het neurale netwerk","text":"<p>Nu het netwerk goed is ge\u00efmplementeerd, kunnen we het gaan trainen. Het idee van die training is dat de waarden in de matrices langzaamaan naar een optimale waarde convergeren, waarmee de voorspelling voor een bepaalde input correspondeert met een goede output (een goede classificatie).</p> <p>Om dit gedaan te krijgen, maken we gebruik van de methode <code>minimize</code> uit <code>scipy.optimize</code>. Deze methode heeft een aantal parameters, waarvan \u00e9\u00e9n het maximaal aantal iteraties is. De methode stopt wanneer het maximaal aantal iteraties is behaald, of het minimale waarde van de kosten heeft gevonden (wat maar het eerst optreedt). Initieel staat deze parameter op 30. Let op: het trainen van het netwerk kan even duren (je krijgt wel het iteratienummer te zien).</p> <p>Je hoeft voor deze opgave niets uit te programmeren. Bestudeer de werking van <code>minimize</code> en experimenteer met verschillende waarden voor de parameter <code>maxiter</code> om een goed beeld te krijgen van de werking van het geheel. Bekijk ook goed hoe de waarden van de matrices worden doorgegeven aan deze methode.</p> <p>Als het geheel is afgerond, worden de nieuwe kost en de nieuwe accuratesse van het netwerk getoond. Ook wordt er een plot gemaakt van de waarden van de matrix in de verborgen laag \u2013 zie de figuur hieronder. Met een beetje moeite en goede wil kun je  zien dat deze matix een gevoeligheid heeft ontwikkeld voor horizontale en verticale lijnen en voor ronde vormen in de input. </p> <p></p> <p>Met de getrainde waarde van het netwerk kunnen we nu voorspellingen doen over het getal dat door een afbeelding wordt gerepresenteerd. Maar de accuratesse is niet de enige metriek die voor de bepaling van hoe goed een netwerk is van belang is. Hierover gaan we het volgende week hebben, wanneer we o.a. de confusion matrix bespreken.</p>"},{"location":"deel2/opgaven/opgave2-3.html","title":"Deel 2 - opgaveset 3","text":""},{"location":"deel2/opgaven/opgave2-3.html#inleiding","title":"Inleiding","text":"<p>Deze set staat in het teken van TensorFlow en Keras. Tot nu toe hebben we de programmacode die bij de wiskunde hoorde zelf uitgeprogrammeerd, maar in de praktijk zul je dat niet heel vaak tegenkomen. Omdat de wiskunde behoorlijk complex kan worden, en feitelijk toch altijd min of meer hetzelfde is, zijn deze twee frameworks ontwikkeld om dergelijke implementatiedetails te abstraheren. Door deze (en vergelijkbare \u2013 er zijn er meer) frameworks te gebruiken, is het voor de ontwikkelaar mogelijk om zich te richten op de daadwerkelijke architectuur en optimalisatie-strategie\u00ebn.</p> <p>In de eerste deelopgave gaan we werken met een standaard-dataset, die in TensorFlow is ingebakken, om een netwerk te trainen. De tweede deelopgave gaat op basis van dit netwerk een confusion matrix uitrekenen en tekenen.</p> <p>De startcode en andere bestanden die bij deze opgave horen kun je hier downloaden. Net als bij de vorige twee sets is er een bestand <code>exercise3.py</code>, dat het bestand <code>uitwerkingen.py</code> gebruikt. Het is de bedoeling dat je dit laatste bestand afmaakt.</p>"},{"location":"deel2/opgaven/opgave2-3.html#opgave-de-fashion-mnist","title":"Opgave: de fashion MNIST","text":"<p>Eerder hebben we gewerkt met de MNIST-dataset, die handschriften van een paar duizend scholieren bevatte. Deze week gebruiken we een andere, vergelijkbare dataset, namelijk de fashion-MNIST. Deze set bevat afbeeldingen van mode-items, zoals broeken, jurken en schoenen. De set bevat 60.000 trainingsplaatjes en 10.000 testplaatjes. Elk plaatje is een 28 \u00d7 28 grayscale plaatje.</p> <p>Het bestand <code>exercise3.py</code> begint met het inladen van een aantal dependencies en laadt vervolgens de dataset in. Deze zit standaard in TensorFlow \u2013 bestudeer het script om hier een beeld van te krijgen. Het laden van de data kan de eerste keer even duren (uiteraard afhankelijk van de snelheid van je downstream). Als de data geladen is, worden de dimensies van de verschillende datasets uitgeprint.</p>"},{"location":"deel2/opgaven/opgave2-3.html#opgave-1a-het-visualiseren-en-prepareren-van-de-data","title":"Opgave 1a: het visualiseren en prepareren van de data","text":"<p>Zoals altijd beginnen we met het visualiseren van de data. Maak de methode <code>plot_image()</code> in <code>uitwerkingen</code> af. Deze methode krijgt een array van 28 \u00d7 28 als parameter mee en maakt gebruik van <code>pyplotlib</code> om hier een plaatje van te tekenen; ook wordt het label dat met het plaatje correspondeert aan de methode meegegeven. Zorg ervoor dat dit label onderaan het plaatje komt te staan. Het script <code>exercise3.py</code> roept deze methode aan met een willekeurige sample uit de dataset, zodat je eenvoudig kunt controleren of het plaatje correspondeert met het label.</p> <p></p> <p>Net als de vorige weken kun je het tekenen van het plaatje vervolgens overslaan door de parameter <code>skip</code> aan het script mee te geven.</p>"},{"location":"deel2/opgaven/opgave2-3.html#opgave-1b-aanpassen-van-data","title":"Opgave 1b: aanpassen van data","text":"<p>Als je de data van de plaatjes bestudeert, zie je dat de getallen waaruit deze zijn opgemaakt liggen in de range van 0-255. Om deze goed door een neuraal netwerk te laten verwerken, is het van belang deze range om te zetten in getallen tussen de nul en de \u00e9\u00e9n. Implementeer de methode <code>scale_data()</code>, zodat de waarden van de getallen in de daaraan meegegeven matrix omgezet worden in de range 0-1. Zorg er daarbij voor, dat hier een willekeurige matrix aan kan worden meegegeven (dus ook \u00e9\u00e9n, waarbij de range van de oorsponkelijke waarden ligt tussen 0 en 1024). Maak gebruik van de numpy-methode amax om het hoogste getal in de meegegeven matrix te bepalen.</p> <p>Als je deze methode hebt ge\u00efmplementeerd, roept het script <code>exercise3.py</code> hem aan, zodat je kunt controleren of het klopt. Vervolgens wordt deze methode aangeroepen met <code>train_images</code> en <code>test_images</code>.</p>"},{"location":"deel2/opgaven/opgave2-3.html#opgave-1c-het-maken-van-het-model","title":"Opgave 1c: Het maken van het model","text":"<p>Nu we de data hebben voorbereid, is het tijd om het model te maken. Tijdens de theorieles is ingegaan op de manier waarop je met Keras moet werken: dat moet je bij deze opgave toepassen. Maak in de methode <code>build_model</code> een netwerk met drie lagen: een input-laag die de plaatjes van 28\u00d728 omzet in 784 input-nodes; een tweede laag van 128 nodes die volledig verbonden is met de input-laag; en een derde laag met tien output-nodes. Geef aan de verborgen middelste laag een tf.nn.relu-activatie mee, en in de output-laag een tf.nn.softmax. </p> <p>Het kan zijn dat je wat <code>deprecation-warnings</code> krijgt bij het aanmaken van dit model (afhankelijk van de versie van TensorFlow die je gebruikt). Die kun je negeren.</p> <p>Het model moet verder voorzien worden van een aantal parameters:</p> <ul> <li> <p>De optimizer, die aangeeft hoe het model wordt ge\u00fcpdate op basis van de data en de loss-function.</p> </li> <li> <p>De loss-function, die aangeeft hoe de accuratesse van het model gedurende de trainingsronden wordt bepaald. </p> </li> <li> <p>De metrics, waarmee de training en de tests worden gemonitord.</p> </li> </ul> <p>Geef deze parameters respectievelijk de waarden <code>adam</code>, <code>sparse_categorical_crossentropy</code>, en <code>accuracy</code>. Bestudeer de documentatie voor nadere specificaties hiervan. Retourneer het model vanuit de methode <code>build_model()</code>.</p> <p>Wanneer deze opgave is afgerond, kun je het script <code>exercise3.py</code> opnieuw runnen. Hier wordt nu de methode <code>fit</code> op het model aangeroepen om het te trainen.</p> <p>In de volgende opgaveset gaan we vervolgens in op het bepalen van de kwaliteit van het getrainde netwerk. Maak hiervoor de methode <code>save_model()</code> af, zodat het getrainde model op een locatie op je lokale computer wordt opgeslagen. Bestudeer eventueel de documentatie om te zien hoe je dit doet.</p>"},{"location":"deel3/inleveren.html","title":"Inleveren deel 3","text":"<ul> <li>Maak de opgavenset over de evaluatie van neurale netwerken</li> <li>Maak de Notebook over modelevaluatie met het <code>helpers.py</code>-bestand</li> <li>Maak de Notebook over DBSCAN</li> </ul>"},{"location":"deel3/week5.html","title":"Week 5: Model-evaluatie, Hyperparameter tuning en Dimensionaliteitsreductie","text":""},{"location":"deel3/week5.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Complexiteit van modellen<ul> <li>Polynomiale modellen</li> <li>De bias-variance tradeoff</li> <li>Over- en underfitting</li> </ul> </li> <li>Model-evaluatie <ul> <li>De confusion matrix</li> <li>True/false positives en negatives</li> <li>Metrieken zoals Precision en Recall</li> <li>De Receiver Operating Characteristic (ROC-curve)</li> <li>Area Under Curve (AUC)</li> </ul> </li> <li>Hyperparameter tuning<ul> <li>GridSearchCV</li> <li>RandomizedSearchCV</li> <li>Halving</li> </ul> </li> <li>Dimensionaliteitsreductie<ul> <li>Projectie en Manifold Learning</li> <li>PCA</li> </ul> </li> </ul>"},{"location":"deel3/week5.html#college-sheets","title":"College-sheets","text":"<ul> <li>Hier vindt u de presentatie die in het college gebruikt is.</li> <li>En hier de Notebook over modelevaluatie.</li> <li>En hier de Notebook over hyperparameter tuning.</li> <li>En hier de Notebook over Principal Components Analysis (PCA).</li> </ul>"},{"location":"deel3/week5.html#lezen","title":"Lezen","text":"<ul> <li>Reader: hoofdstuk 9</li> <li>G\u00e9ron: hoofdstuk 2 pp. 91-94 en hoofdstuk 8</li> <li>G\u00e9ron: hoofdstuk 3, pp. 108-118</li> </ul>"},{"location":"deel3/week6.html","title":"Week 6: Andere modellen en Ensemble learning","text":""},{"location":"deel3/week6.html#onderwerpen","title":"Onderwerpen","text":"<ul> <li>Supervised<ul> <li>Naive Bayes classifier</li> <li>Support Vector Classifiers (SVC's)</li> <li>Beslisbomen (Decision trees)</li> </ul> </li> <li>Unsupervised<ul> <li>DBSCAN</li> <li>k-means</li> </ul> </li> <li>Ensemble learning<ul> <li>Voting, bagging en pasting</li> <li>Random forest</li> <li>Boosting</li> <li>Stacking</li> </ul> </li> </ul>"},{"location":"deel3/week6.html#college-sheets","title":"College-sheets","text":"<ul> <li>Hier vindt u de presentatie die in het college gebruikt is.</li> <li>En hier een Notebook over Decisions trees (niet getoond in HC).</li> <li>En hier de Notebook over kMeans.</li> <li>En hier de Notebook over DBSCAN.</li> <li>En hier de Notebook over Random forests (niet getoond in HC).</li> <li>En hier de Notebook over Voting.</li> </ul>"},{"location":"deel3/week6.html#lezen","title":"Lezen","text":"<ul> <li>G\u00e9ron: hoofdstuk 5, 6, 7 en 9 (Clustering algorithms)</li> </ul>"},{"location":"deel3/opgaven/opgave3-1.html","title":"Deel 3 - opgaveset 1","text":"<p>In deze opgave gaan we op basis van de test-data bepalen hoe goed ons eerder getrainde netwerk is. Download hiervoor de opgaven vanuit deze zip. </p> <p>Zoals bekend is de accuratesse op zich niet voldoende om de presentatie van een classifier te bepalen: wanneer je gewoon zou gokken heb je bij tien mogelijke categorie\u00ebn al een score van tien procent, en als je zou gokken dat een sample iets niet is, is de accuratesse al negentig procent. Betere metrieken hiervoor worden gegeven door de confusion matrix, die tijdens de theorieles besproken is. </p> <p></p> <p>Maak eerst de methode <code>load_model()</code> af; deze methode moet het getrainde model laden dat je als laatste stap in de vorige opgave hebt opgeslagen.</p>"},{"location":"deel3/opgaven/opgave3-1.html#opgave-1a-bepalen-van-de-confusion-matrix","title":"Opgave 1a: bepalen van de confusion matrix","text":"<p>Maak de methode <code>conf_matrix()</code> af. Zoals je ziet krijgt deze methode twee parameters mee, namelijk de door het netwerk voorspelde waarden, en de daadwerkelijke waarden. Maak gebruik van de methode <code>confusion_matrix</code> in TensorFlow om deze matrix te bepalen. Retourneer de matrix.</p> <p>Als je hiermee klaar bent, wordt de methode door het script <code>exercise3.py</code> aangeroepen, met de voorspellingen van de <code>test_images</code> en actuele waarden van die test-data (<code>test_labels</code>). Het resultaat wordt vervolgens in een plot weergegeven; als het goed is, ziet het er ongeveer als volgt uit: </p> <p></p>"},{"location":"deel3/opgaven/opgave3-1.html#opgave-1b-tp-tn-fp-fn","title":"Opgave 1b: TP, TN, FP, FN","text":"<p>Om meer metrieken uit het getrainde model te halen, volstaat niet alleen het percentage van de samples dat correct is geclassificeerd; we moeten dan ook weten welk percentage terecht als niet van een bepaalde klasse is geclassificeerd, welke onterecht als wel van een bepaalde klasse, en welk percentage onterecht als wel van een bepaalde klasse: de zogenaamde true positives, true negatives, false positives en false negatives (zoals in het theoriecollege besproken is).</p> <p>De methode <code>conf_els()</code> in <code>uitwerkingen.py</code> krijgt als parameter een numpy-array mee, die ook ten grondslag ligt aan de afbeelding hierboven. De regels van deze matrix corresponderen met de werkelijke waarde van het sample, de kolommen van deze matrix corresponderen met de voorspelling van het sample door het model. Hoewel er semantisch wel het \u00e9\u00e9n en ander op aan te merken is, defini\u00ebren we de hierboven beschreven metrieken als volgt:</p> <ul> <li> <p>\\(tp_{i} = c_{ii}\\)</p> </li> <li> <p>\\(fp_{i} = \\sum_{l=1}^n c_{li} - tp_{i}\\)</p> </li> <li> <p>\\(fn_{i} = \\sum_{l=1}^n c_{il} - tp_{i}\\)</p> </li> <li> <p>\\(tn_{i} = \\sum_{l=1}^n \\sum_{k=1}^n c_{lk} -tp_{i} - fp_{i} - fn_{i}\\)</p> </li> </ul> <p>Hierbij is \\(i\\) de categorie in kwestie (dus in dit specifieke geval loopt die van 1 - 10). </p> <p>Implementeer de methode <code>conf_els()</code>, en retourneer een lijst met deze vier metrieken voor elk label \u2013 bestudeer het reeds gegeven deel van de implementatie om een beeld te krijgen van de exacte vorm van de return-waarde.</p>"},{"location":"deel3/opgaven/opgave3-1.html#opgave-1c-precision-en-recall","title":"Opgave 1c: precision en recall","text":"<p>Implementeer nu de methode <code>conf_data()</code>, waarin je de data die je in de vorige opgave hebt gemaakt omzet in de onderstaande metrieken: </p> <ul> <li> <p>\\(precision (PPV) = \\frac{tp}{tp + fp}\\)</p> </li> <li> <p>\\(recall (sensitivity, TPR) = \\frac{tp}{tp + fn}\\)</p> </li> <li> <p>\\(specificity (TNR) = \\frac{tn}{tn + fp}\\)</p> </li> <li> <p>\\(fall-out (FPR) = \\frac{fp}{fp + tn}\\)</p> </li> </ul> <p>Deze methode krijgt de lijst uit de vorige opgave mee: de totale \\(tp\\) is dan de som van alle \\(tp\\)'s van alle labels \u2013 en vergelijkbare berekeningen voor de total \\(tn\\), \\(fp\\) en \\(fn\\). Retourneer deze data als een dictionary. Als je deze beide methoden hebt ge\u00efmplementeerd, kun je het script <code>exercise3.py</code> nogmaals runnen; Hierdoor worden deze waarden afgedrukt. Zeg op basis van deze resultaten iets over de kwaliteit van het uitgeprogrammeerde netwerk.</p>"},{"location":"deel4/inleveren.html","title":"Inleveren deel 4","text":"<ul> <li> <p>Maak de notebook over word-embeddings en PCA. Hiervoor zijn de volgende bestanden van belang:</p> <ul> <li><code>utils.py</code></li> <li><code>capitals.txt</code></li> <li><code>word_embeddings_subset.p</code></li> <li><code>vectors.png</code></li> <li><code>word_embf.png</code></li> </ul> </li> <li> <p>Maak de notebook over n-grams. Hiervoor zijn de volgende bestanden van belang:</p> <ul> <li><code>n_gram.png</code></li> <li><code>wiki.txt</code></li> </ul> </li> <li> <p>Maak de notebook over de Goldberg-Variationen. Hiervoor zijn de volgende zip en py-bestanden nodig:</p> <ul> <li><code>bwv988.zip</code></li> <li><code>deprocess.py</code></li> <li><code>csvtomidi.py</code></li> <li><code>encoding.json</code></li> </ul> </li> </ul>"},{"location":"deel4/week7.html","title":"Week 7: Taalmodellen","text":""},{"location":"deel4/week7.html#onderwerpen","title":"Onderwerpen","text":"<p>NB: omdat het hoorcollege van week 8 erg onhandig geroosterd is, komt dit te vervallen en wordt de stof van week 7 \u00e9n 8 in het hoorcollege van week 7 behandeld.</p> <ul> <li>(Large) Language Models</li> <li>Word Embeddings</li> <li>Recurrente neurale netwerken (RNN's)</li> <li>RNN's met geheugen (LSTM, GRU)</li> <li>Transformers</li> <li>Ruimte voor vragen over de eerdere stof en de laatste opdracht</li> <li>Afsluiting en evaluatie van het vak</li> </ul>"},{"location":"deel4/week7.html#college-sheets","title":"College-sheets","text":"<ul> <li>Hier vindt u de presentatie die in het college gebruikt is.</li> <li>En hier de Notebook waarin we geprobeerd hebben een science fiction-roman te schrijven op basis van character prediction met een RNN. NB: dat er naast de vectorisatie ook een Embedding-laag in het definitieve model zit, is correct en nodig om de als int's gecodeerde karakters om te zetten in vectoren van floats die (beter) door het netwerk verwerkt kunnen worden.</li> </ul>"},{"location":"deel4/week7.html#lezen","title":"Lezen","text":"<ul> <li>G\u00e9ron: hoofdstuk 15 en 16</li> </ul>"},{"location":"deel4/week8.html","title":"Week 8: Recurrente neurale netwerken","text":""},{"location":"deel4/week8.html#onderwerpen","title":"Onderwerpen","text":"<p>NB: omdat het hoorcollege van week 8 erg onhandig geroosterd is, komt dit te vervallen en wordt de stof van week 7 \u00e9n 8 in het hoorcollege van week 7 behandeld.</p>"}]}